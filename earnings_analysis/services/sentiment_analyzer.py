# earnings_analysis/services/sentiment_analyzer.pyï¼ˆã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆä¿®æ­£ç‰ˆï¼‰
import re
import csv
import os
import threading
import time
import logging
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass
from django.conf import settings
from django.core.cache import cache
from django.utils import timezone
from datetime import timedelta

from .xbrl_extractor import EDINETXBRLService

logger = logging.getLogger(__name__)

@dataclass
class AnalysisConfig:
    """æ„Ÿæƒ…åˆ†æè¨­å®š"""
    positive_threshold: float = 0.15  # é–¾å€¤ã‚’ä¸‹ã’ã¦ã‚ˆã‚Šå¤šãã®èªå½™ã‚’æ¤œå‡º
    negative_threshold: float = -0.15  # é–¾å€¤ã‚’ä¸‹ã’ã¦ã‚ˆã‚Šå¤šãã®èªå½™ã‚’æ¤œå‡º
    min_sentence_length: int = 10  # æœ€å°æ–‡é•·ã‚’çŸ­ãã—ã¦æ–‡ç« ã‚’å–å¾—ã—ã‚„ã™ã
    max_sample_sentences: int = 15  # ã‚µãƒ³ãƒ—ãƒ«æ–‡ç« æ•°ã‚’å¢—åŠ 
    cache_timeout: int = 3600
    min_numeric_value: float = 5.0
    context_window: int = 5


class TransparentSentimentDictionary:
    """åˆ†ã‹ã‚Šã‚„ã™ã„æ„Ÿæƒ…è¾æ›¸ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, dict_path: Optional[str] = None):
        self.dict_path = dict_path or getattr(
            settings, 'SENTIMENT_DICT_PATH', 
            os.path.join(settings.BASE_DIR, 'data', 'sentiment_dict.csv')
        )
        self.sentiment_dict = {}
        self.improvement_patterns = []
        self.deterioration_patterns = []
        self.negation_patterns = []
        self._last_modified = 0
        self.load_dictionary()
    
    def load_dictionary(self) -> None:
        """æ„Ÿæƒ…è¾æ›¸ã®èª­ã¿è¾¼ã¿ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        if os.path.exists(self.dict_path):
            try:
                self._load_from_file()
                self._build_patterns()
                logger.info(f"æ„Ÿæƒ…è¾æ›¸èª­ã¿è¾¼ã¿å®Œäº†: {len(self.sentiment_dict)}èª")
                
                # ãƒ‡ãƒãƒƒã‚°ï¼šè¾æ›¸ã®ä¸€éƒ¨ã‚’ãƒ­ã‚°å‡ºåŠ›
                sample_items = list(self.sentiment_dict.items())[:10]
                logger.info(f"è¾æ›¸ã‚µãƒ³ãƒ—ãƒ«: {sample_items}")
                
            except Exception as e:
                logger.error(f"æ„Ÿæƒ…è¾æ›¸èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                self._load_default_dictionary()
        else:
            logger.warning(f"æ„Ÿæƒ…è¾æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.dict_path}")
            self._load_default_dictionary()
    
    def _load_from_file(self) -> None:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®è¾æ›¸èª­ã¿è¾¼ã¿ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        loaded_count = 0
        
        try:
            with open(self.dict_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼ç¢ºèª
                fieldnames = reader.fieldnames
                logger.info(f"CSVãƒ˜ãƒƒãƒ€ãƒ¼: {fieldnames}")
                
                for row_num, row in enumerate(reader, 1):
                    try:
                        # èªå½™ã¨ã‚¹ã‚³ã‚¢ã‚’å–å¾—
                        word = row.get('word', '').strip()
                        score_str = row.get('score', '').strip()
                        
                        if not word or not score_str:
                            logger.debug(f"è¡Œ{row_num}: ç©ºã®å€¤ã‚’ã‚¹ã‚­ãƒƒãƒ— - word='{word}', score='{score_str}'")
                            continue
                        
                        # ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
                        if word.startswith('#'):
                            continue
                        
                        # ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–ï¼ˆå…¨è§’ãƒ»åŠè§’ã®æ•°å­—ã€ãƒã‚¤ãƒŠã‚¹è¨˜å·ã®çµ±ä¸€ï¼‰
                        score_str = score_str.replace('âˆ’', '-').replace('ï¼', '-')
                        score_str = score_str.replace('ï¼‘', '1').replace('ï¼’', '2').replace('ï¼“', '3')
                        score_str = score_str.replace('ï¼”', '4').replace('ï¼•', '5').replace('ï¼–', '6')
                        score_str = score_str.replace('ï¼—', '7').replace('ï¼˜', '8').replace('ï¼™', '9')
                        score_str = score_str.replace('ï¼', '0').replace('ï¼', '.')
                        
                        score = float(score_str)
                        
                        # ã‚¹ã‚³ã‚¢ç¯„å›²ãƒã‚§ãƒƒã‚¯
                        if not (-1.0 <= score <= 1.0):
                            logger.warning(f"è¡Œ{row_num}: ã‚¹ã‚³ã‚¢ç¯„å›²å¤– - {word}: {score}")
                            continue
                        
                        # è¾æ›¸ã«è¿½åŠ 
                        self.sentiment_dict[word] = score
                        loaded_count += 1
                        
                        # æœ€åˆã®æ•°ä»¶ã‚’ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
                        if loaded_count <= 5:
                            logger.info(f"èªå½™ç™»éŒ²: '{word}' â†’ {score}")
                            
                    except (ValueError, KeyError) as e:
                        logger.warning(f"è¡Œ{row_num}: è§£æã‚¨ãƒ©ãƒ¼ - {row} â†’ {e}")
                        continue
                
                logger.info(f"è¾æ›¸èª­ã¿è¾¼ã¿å®Œäº†: {loaded_count}èªã‚’ç™»éŒ²")
                
        except Exception as e:
            logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _build_patterns(self) -> None:
        """æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ§‹ç¯‰ï¼ˆä¿®æ­£ç‰ˆï¼‰"""
        # æ”¹å–„ã‚’è¡¨ã™ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒã‚¬ãƒ†ã‚£ãƒ–â†’ãƒã‚¸ãƒ†ã‚£ãƒ–è»¢æ›ï¼‰
        self.improvement_patterns = [
            r'(æ¸›å|æ¸›ç›Š|èµ¤å­—|æå¤±|æ¥­ç¸¾æ‚ªåŒ–|ä½è¿·|ä¸æŒ¯|è‹¦æˆ¦)(?:ã®|å¹…ã®|å¹…)?(æ”¹å–„|å›å¾©|ç¸®å°|è§£æ¶ˆ|è„±å´|å…‹æœ)',
            r'(æ¸›å|æ¸›ç›Š|èµ¤å­—|æå¤±)(?:ã®|å¹…ã®|å¹…)?ç¸®å°',
            r'(æ¥­ç¸¾æ‚ªåŒ–|ä½è¿·|ä¸æŒ¯)(?:ã‹ã‚‰ã®|ã‹ã‚‰)(å›å¾©|è„±å´|æ”¹å–„)',
            r'(æ‚ªåŒ–|ä½è¿·|ä¸æŒ¯)(?:ã«|ã¸ã®)æ­¯æ­¢ã‚',
            r'ç„¡é…ã‹ã‚‰ã®å¾©é…',
            r'èµ¤å­—ã‹ã‚‰ã®é»’å­—è»¢æ›',
        ]
        
        # æ‚ªåŒ–ã‚’è¡¨ã™ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒã‚¸ãƒ†ã‚£ãƒ–â†’ãƒã‚¬ãƒ†ã‚£ãƒ–è»¢æ›ï¼‰
        self.deterioration_patterns = [
            r'(å¢—å|å¢—ç›Š|æˆé•·|å¥½èª¿|å›å¾©)(?:ã®|ã«)(éˆåŒ–|é ­æ‰“ã¡|ä¸€æœ|é™°ã‚Š)',
            r'(å¢—å|å¢—ç›Š|æˆé•·|æ”¹å–„)(?:ã®|ãŒ)(é…ã‚Œ|è¶³è¸ã¿)',
            r'(å¥½èª¿|é †èª¿)(?:ã«|ãª)(é™°ã‚Š|ä¸€æœ)',
        ]
        
        # å¦å®šãƒ‘ã‚¿ãƒ¼ãƒ³
        self.negation_patterns = [
            r'(æ¸›å|æ¸›ç›Š|èµ¤å­—|æå¤±|æ‚ªåŒ–|ä½è¿·|ä¸æŒ¯)(?:ã§|ã§ã¯)?(ã¯?ãª)(ã„|ã)',
            r'(æ¸›å|æ¸›ç›Š|èµ¤å­—|æå¤±|æ‚ªåŒ–|ä½è¿·|ä¸æŒ¯)(?:ã¨ã„ã†)?(?:ã‚ã‘)?(ã§|ã§ã¯)?(ã¯?ãª)(ã„|ã)',
        ]
        
        logger.info(f"æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³æ§‹ç¯‰å®Œäº†: æ”¹å–„{len(self.improvement_patterns)}å€‹, "
                   f"æ‚ªåŒ–{len(self.deterioration_patterns)}å€‹, "
                   f"å¦å®š{len(self.negation_patterns)}å€‹")
    
    def get_word_score(self, word: str) -> Optional[float]:
        """èªå½™ã®ã‚¹ã‚³ã‚¢å–å¾—ï¼ˆãƒ‡ãƒãƒƒã‚°ä»˜ãï¼‰"""
        score = self.sentiment_dict.get(word)
        if score is not None:
            logger.debug(f"èªå½™ã‚¹ã‚³ã‚¢å–å¾—: '{word}' â†’ {score}")
        return score
    
    def search_words(self, text: str) -> List[Tuple[str, float]]:
        """ãƒ†ã‚­ã‚¹ãƒˆå†…ã®æ„Ÿæƒ…èªå½™ã‚’æ¤œç´¢ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        found_words = []
        for word, score in self.sentiment_dict.items():
            if word in text:
                count = text.count(word)
                found_words.append((word, score, count))
                logger.debug(f"èªå½™ç™ºè¦‹: '{word}' (ã‚¹ã‚³ã‚¢: {score}, å‡ºç¾: {count}å›)")
        
        return found_words
    
    def _load_default_dictionary(self) -> None:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¾æ›¸ï¼ˆæ‹¡å¼µç‰ˆãƒ»ãƒ‡ãƒãƒƒã‚°ä»˜ãï¼‰"""
        logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¾æ›¸ã‚’ä½¿ç”¨ã—ã¾ã™")
        
        self.sentiment_dict = {
            # ãƒã‚¸ãƒ†ã‚£ãƒ–èªå½™
            'å¢—å': 0.8, 'å¢—ç›Š': 0.8, 'å¤§å¹…å¢—å': 0.9, 'å¤§å¹…å¢—ç›Š': 0.9,
            'éå»æœ€é«˜ç›Š': 0.9, 'æœ€é«˜ç›Š': 0.9, 'é»’å­—è»¢æ›': 0.9, 'é»’å­—åŒ–': 0.8,
            'Vå­—å›å¾©': 0.9, 'å¾©é…': 0.8, 'æ”¹å–„': 0.7, 'å‘ä¸Š': 0.7, 'å›å¾©': 0.6, 
            'å¥½èª¿': 0.8, 'é †èª¿': 0.7, 'æˆé•·': 0.8, 'æ‹¡å¤§': 0.6, 'ä¸Šæ˜‡': 0.6, 
            'é”æˆ': 0.7, 'æˆåŠŸ': 0.8, 'åŠ¹ç‡åŒ–': 0.5, 'å¼·åŒ–': 0.6, 'å …èª¿': 0.6,
            
            # æ”¹å–„ãƒ‘ã‚¿ãƒ¼ãƒ³
            'æ¸›åã®æ”¹å–„': 0.7, 'èµ¤å­—ç¸®å°': 0.8, 'æå¤±ã®æ”¹å–„': 0.7,
            'æ¸›åå¹…ã®ç¸®å°': 0.7, 'æ¸›ç›Šã®æ”¹å–„': 0.7, 'æ¥­ç¸¾å‘ä¸Š': 0.7,
            
            # ãƒã‚¬ãƒ†ã‚£ãƒ–èªå½™
            'æ¸›å': -0.7, 'æ¸›ç›Š': -0.8, 'å¤§å¹…æ¸›å': -0.9, 'å¤§å¹…æ¸›ç›Š': -0.9,
            'èµ¤å­—': -0.8, 'èµ¤å­—è»¢è½': -0.9, 'æå¤±': -0.7, 'å–¶æ¥­æå¤±': -0.8,
            'æ‚ªåŒ–': -0.8, 'ä½ä¸‹': -0.6, 'æ¸›å°‘': -0.6, 'ä½è¿·': -0.7, 'ä¸æŒ¯': -0.7,
            'è‹¦æˆ¦': -0.7, 'å›°é›£': -0.7, 'å³ã—ã„': -0.6, 'ä¸‹è½': -0.6,
            
            # æ‚ªåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³
            'å¢—åã®éˆåŒ–': -0.5, 'æˆé•·ã®éˆåŒ–': -0.6, 'å¥½èª¿ã«é™°ã‚Š': -0.5,
            
            # ä¸­ç«‹
            'ç¶­æŒ': 0.1, 'ç¶™ç¶š': 0.2, 'æ¨ç§»': 0.0, 'äºˆæƒ³': 0.0,
        }
        
        logger.info(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¾æ›¸æ§‹ç¯‰å®Œäº†: {len(self.sentiment_dict)}èª")
        self._build_patterns()
        

class TransparentTextProcessor:
    """åˆ†ã‹ã‚Šã‚„ã™ã„ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    @staticmethod
    def preprocess(text: str) -> str:
        """æ”¹è‰¯ç‰ˆãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ï¼ˆæ•°å€¤ä¿æŒï¼‰"""
        if not text:
            return ""
        
        # HTMLã‚¿ã‚°é™¤å»
        text = re.sub(r'<[^>]+>', '', text)
        
        # é‡è¦ãªé‡‘èè¡¨ç¾ã‚’ä¿è­·
        protected_patterns = []
        financial_patterns = [
            # æ”¹å–„ãƒ»æ‚ªåŒ–è¡¨ç¾ï¼ˆæ•°å€¤ä»˜ãï¼‰
            (r'(æ¸›å|æ¸›ç›Š|èµ¤å­—|æå¤±)(?:ã®|å¹…ã®|å¹…)?(?:ãŒ|ã¯)?\d+(?:\.\d+)?[ï¼…%]?(?:ã®)?(æ”¹å–„|ç¸®å°)', 'IMPROVEMENT'),
            (r'(å¢—å|å¢—ç›Š|æˆé•·)(?:ã®|ãŒ)?\d+(?:\.\d+)?[ï¼…%]?(?:ã®)?(éˆåŒ–|é ­æ‰“ã¡)', 'DETERIORATION'),
            
            # åŸºæœ¬çš„ãªæ”¹å–„ãƒ»æ‚ªåŒ–è¡¨ç¾
            (r'(æ¸›å|æ¸›ç›Š|èµ¤å­—|æå¤±|æ¥­ç¸¾æ‚ªåŒ–|ä½è¿·|ä¸æŒ¯)(?:ã®|å¹…ã®|å¹…)?(æ”¹å–„|å›å¾©|ç¸®å°|è§£æ¶ˆ|è„±å´)', 'IMPROVEMENT'),
            (r'(å¢—å|å¢—ç›Š|æˆé•·|å¥½èª¿)(?:ã®|ã«)(éˆåŒ–|é ­æ‰“ã¡|ä¸€æœ|é™°ã‚Š)', 'DETERIORATION'),
            
            # ç‰¹åˆ¥ãªè¡¨ç¾
            (r'Vå­—å›å¾©', 'RECOVERY'),
            (r'é»’å­—è»¢æ›', 'PROFIT_CHANGE'),
            (r'èµ¤å­—è»¢è½', 'LOSS_CHANGE'),
            
            # æ•°å€¤è¡¨ç¾ï¼ˆé‡è¦ãªã‚‚ã®ã®ã¿ä¿è­·ï¼‰
            (r'\d+(?:\.\d+)?(?:ï¼…|%|å€)(?:ä»¥ä¸Š|è¶…|å¢—|æ¸›|ä¸Šæ˜‡|ä¸‹è½|æ”¹å–„|æ‚ªåŒ–)', 'NUMERIC'),
            (r'(?:éå»|)\d+å¹´(?:ã¶ã‚Š|é€£ç¶š)', 'PERIOD'),
        ]
        
        for i, (pattern, prefix) in enumerate(financial_patterns):
            for match in re.finditer(pattern, text):
                placeholder = f"__{prefix}_{i}__"
                protected_patterns.append((placeholder, match.group()))
                text = text.replace(match.group(), placeholder, 1)
        
        # ä¸€èˆ¬çš„ãªæ•´ç†ï¼ˆæ•°å€¤ã¯ä¿æŒï¼‰
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[ã€ã€‘ã€Œã€ï¼ˆï¼‰\(\)\[\]ã€”ã€•]', '', text)
        
        # ä¿è­·ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¾©å…ƒ
        for placeholder, original in protected_patterns:
            text = text.replace(placeholder, original)
        
        return text.strip()


class UserInsightGenerator:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘è¦‹è§£ç”Ÿæˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.business_terms = {
            'positive': [
                'æˆé•·æˆ¦ç•¥', 'åç›Šæ”¹å–„', 'ç«¶äº‰åŠ›å¼·åŒ–', 'å¸‚å ´æ‹¡å¤§', 'åŠ¹ç‡åŒ–',
                'æ¥­ç¸¾å‘ä¸Š', 'æ ªä¸»ä¾¡å€¤', 'æŒç¶šçš„æˆé•·', 'æŠ€è¡“é©æ–°', 'æ–°è¦äº‹æ¥­'
            ],
            'negative': [
                'ãƒªã‚¹ã‚¯ç®¡ç†', 'èª²é¡Œå¯¾å¿œ', 'æ§‹é€ æ”¹é©', 'æ¥­ç¸¾æ”¹å–„', 'ã‚³ã‚¹ãƒˆå‰Šæ¸›',
                'å¸‚å ´å¤‰åŒ–', 'ç«¶äº‰æ¿€åŒ–', 'ä¸ç¢ºå®Ÿæ€§', 'çµŒå–¶èª²é¡Œ', 'äº‹æ¥­å†ç·¨'
            ]
        }
    
    def generate_detailed_insights(self, analysis_result: Dict[str, Any], document_info: Dict[str, str]) -> Dict[str, Any]:
        """è©³ç´°ãªè¦‹è§£ã‚’ç”Ÿæˆ"""
        overall_score = analysis_result.get('overall_score', 0)
        sentiment_label = analysis_result.get('sentiment_label', 'neutral')
        statistics = analysis_result.get('statistics', {})
        keyword_analysis = analysis_result.get('keyword_analysis', {})
        
        insights = {
            'market_implications': self._generate_market_implications(overall_score, sentiment_label, keyword_analysis),
            'business_strategy_reading': self._generate_business_strategy_reading(analysis_result, document_info),
            'investor_perspective': self._generate_investor_perspective(overall_score, sentiment_label, statistics),
            'risk_assessment': self._generate_risk_assessment(analysis_result),
            'competitive_position': self._generate_competitive_analysis(keyword_analysis, overall_score),
            'future_outlook': self._generate_future_outlook(analysis_result),
            'stakeholder_recommendations': self._generate_stakeholder_recommendations(overall_score, sentiment_label, statistics)
        }
        
        return insights
    
    def _generate_market_implications(self, score: float, label: str, keywords: Dict) -> Dict[str, Any]:
        """å¸‚å ´ã¸ã®å½±éŸ¿åˆ†æ"""
        implications = {
            'market_sentiment': '',
            'stock_impact_likelihood': '',
            'sector_comparison': '',
            'timing_considerations': ''
        }
        
        if label == 'positive':
            if score > 0.6:
                implications['market_sentiment'] = 'éå¸¸ã«ãƒã‚¸ãƒ†ã‚£ãƒ–ãªå¸‚å ´åå¿œãŒæœŸå¾…ã•ã‚Œã‚‹å†…å®¹ã§ã™ã€‚'
                implications['stock_impact_likelihood'] = 'é«˜ã„ç¢ºç‡ã§æ ªä¾¡ã«ãƒ—ãƒ©ã‚¹ã®å½±éŸ¿ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚'
            else:
                implications['market_sentiment'] = 'å¸‚å ´ã«å¯¾ã—ã¦å‰å‘ããªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç™ºä¿¡ã—ã¦ã„ã¾ã™ã€‚'
                implications['stock_impact_likelihood'] = 'çŸ­æœŸçš„ã«ã¯ãƒ—ãƒ©ã‚¹ææ–™ã¨ã—ã¦è©•ä¾¡ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚'
        elif label == 'negative':
            if score < -0.6:
                implications['market_sentiment'] = 'å¸‚å ´ã®æ…é‡ãªåå¿œãŒäºˆæƒ³ã•ã‚Œã‚‹å†…å®¹ã§ã™ã€‚'
                implications['stock_impact_likelihood'] = 'ä¸€æ™‚çš„ãªæ ªä¾¡ä¸‹è½è¦å› ã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚'
            else:
                implications['market_sentiment'] = 'å¸‚å ´ã¯ä¼æ¥­ã®é€æ˜æ€§ã‚’è©•ä¾¡ã™ã‚‹ä¸€æ–¹ã€æ…é‡ãªå§¿å‹¢ã‚’è¦‹ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚'
                implications['stock_impact_likelihood'] = 'çŸ­æœŸçš„ãªå½±éŸ¿ã¯é™å®šçš„ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚'
        else:
            implications['market_sentiment'] = 'å¸‚å ´åå¿œã¯ä¸­ç«‹çš„ã§ã€ä»–ã®è¦å› ã«ã‚ˆã‚Šå·¦å³ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚'
            implications['stock_impact_likelihood'] = 'æ„Ÿæƒ…çš„ãªææ–™ã¨ã—ã¦ã®å½±éŸ¿ã¯é™å®šçš„ã¨äºˆæƒ³ã•ã‚Œã¾ã™ã€‚'
        
        return implications
    
    def _generate_business_strategy_reading(self, analysis_result: Dict, document_info: Dict) -> Dict[str, str]:
        """çµŒå–¶æˆ¦ç•¥ã®èª­ã¿å–ã‚Š"""
        strategy_reading = {
            'management_stance': '',
            'strategic_direction': '',
            'operational_focus': ''
        }
        
        keyword_analysis = analysis_result.get('keyword_analysis', {})
        positive_keywords = keyword_analysis.get('positive', [])
        negative_keywords = keyword_analysis.get('negative', [])
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æˆ¦ç•¥ã‚’èª­ã¿å–ã‚Š
        growth_words = [kw for kw in positive_keywords if any(term in kw.get('word', '') for term in ['æˆé•·', 'æ‹¡å¤§', 'å¢—å', 'å¢—ç›Š'])]
        improvement_words = [kw for kw in positive_keywords if any(term in kw.get('word', '') for term in ['æ”¹å–„', 'å‘ä¸Š', 'åŠ¹ç‡', 'å¼·åŒ–'])]
        
        if growth_words:
            strategy_reading['strategic_direction'] = 'æˆé•·å¿—å‘ã®æˆ¦ç•¥ãŒæ˜ç¢ºã«ç¤ºã•ã‚Œã¦ãŠã‚Šã€äº‹æ¥­æ‹¡å¤§ã¸ã®ç©æ¥µçš„ãªå§¿å‹¢ãŒèª­ã¿å–ã‚Œã¾ã™ã€‚'
        elif improvement_words:
            strategy_reading['strategic_direction'] = 'åŠ¹ç‡æ€§ã¨å“è³ªå‘ä¸Šã«é‡ç‚¹ã‚’ç½®ã„ãŸæˆ¦ç•¥ãŒå±•é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚'
        
        # ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ãƒªã‚¹ã‚¯å¯¾å¿œã‚’èª­ã¿å–ã‚Š
        risk_words = [kw for kw in negative_keywords if any(term in kw.get('word', '') for term in ['ãƒªã‚¹ã‚¯', 'èª²é¡Œ', 'å›°é›£', 'å³ã—ã„'])]
        
        if risk_words:
            strategy_reading['management_stance'] = 'ãƒªã‚¹ã‚¯ã‚’æ­£é¢ã‹ã‚‰æ‰ãˆã€èª²é¡Œè§£æ±ºã«å‘ã‘ãŸç¾å®Ÿçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚'
        else:
            strategy_reading['management_stance'] = 'å®‰å®šã—ãŸçµŒå–¶åŸºç›¤ã®ä¸Šã«ã€ç€å®Ÿãªäº‹æ¥­é‹å–¶ã‚’è¡Œã£ã¦ã„ã¾ã™ã€‚'
        
        return strategy_reading
    
    def _generate_investor_perspective(self, score: float, label: str, statistics: Dict) -> Dict[str, str]:
        """æŠ•è³‡å®¶è¦–ç‚¹ã§ã®åˆ†æ"""
        investor_view = {
            'investment_appeal': '',
            'risk_reward_balance': '',
            'dividend_outlook': '',
            'growth_potential': ''
        }
        
        total_words = statistics.get('total_words_analyzed', 0)
        
        if label == 'positive':
            investor_view['investment_appeal'] = 'æŠ•è³‡é­…åŠ›åº¦ã¯é«˜ãã€æˆé•·æœŸå¾…ã‚’æŒã¦ã‚‹ä¼æ¥­ã¨ã—ã¦ä½ç½®ã¥ã‘ã‚‰ã‚Œã¾ã™ã€‚'
            investor_view['growth_potential'] = 'ä¸­é•·æœŸçš„ãªæˆé•·ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ãŒæœŸå¾…ã§ãã‚‹å†…å®¹ã¨ãªã£ã¦ã„ã¾ã™ã€‚'
            if score > 0.5:
                investor_view['dividend_outlook'] = 'æ ªä¸»é‚„å…ƒç­–ã®æ‹¡å……ã‚„å¢—é…ã®å¯èƒ½æ€§ã‚‚æœŸå¾…ã•ã‚Œã¾ã™ã€‚'
        elif label == 'negative':
            investor_view['investment_appeal'] = 'ãƒªã‚¹ã‚¯ã‚’æ…é‡ã«è©•ä¾¡ã—ãŸä¸Šã§ã®æŠ•è³‡åˆ¤æ–­ãŒå¿…è¦ã§ã™ã€‚'
            investor_view['risk_reward_balance'] = 'ãƒªã‚¹ã‚¯ã¯å­˜åœ¨ã—ã¾ã™ãŒã€ãã‚Œã«è¦‹åˆã£ãŸãƒªã‚¿ãƒ¼ãƒ³ã®å¯èƒ½æ€§ã‚‚ã‚ã‚Šã¾ã™ã€‚'
        else:
            investor_view['investment_appeal'] = 'å®‰å®šã—ãŸæŠ•è³‡å…ˆã¨ã—ã¦ã€ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚·ãƒ–ãªæŠ•è³‡æˆ¦ç•¥ã«é©ã—ã¦ã„ã¾ã™ã€‚'
            investor_view['growth_potential'] = 'æ€¥æˆé•·ã¯æœŸå¾…ã§ãã¾ã›ã‚“ãŒã€å®‰å®šã—ãŸæˆé•·ãŒè¦‹è¾¼ã¾ã‚Œã¾ã™ã€‚'
        
        if total_words > 50:
            investor_view['analysis_reliability'] = f'ååˆ†ãªæƒ…å ±é‡ï¼ˆ{total_words}èªï¼‰ã«åŸºã¥ãåˆ†æã®ãŸã‚ã€ä¿¡é ¼æ€§ã¯é«˜ã„ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚'
        
        return investor_view
    
    def _generate_risk_assessment(self, analysis_result: Dict) -> Dict[str, Any]:
        """ãƒªã‚¹ã‚¯è©•ä¾¡"""
        risk_assessment = {
            'identified_risks': [],
            'risk_level': 'medium',
            'mitigation_evidence': [],
            'monitoring_points': []
        }
        
        negative_keywords = analysis_result.get('keyword_analysis', {}).get('negative', [])
        negative_sentences = analysis_result.get('sample_sentences', {}).get('negative', [])
        
        # ãƒªã‚¹ã‚¯ã®ç‰¹å®š
        for keyword in negative_keywords:
            word = keyword.get('word', '')
            if any(risk_term in word for risk_term in ['ãƒªã‚¹ã‚¯', 'æ¸›å', 'æ¸›ç›Š', 'æå¤±', 'å›°é›£']):
                risk_assessment['identified_risks'].append(f"{word}ã«é–¢ã™ã‚‹ãƒªã‚¹ã‚¯")
        
        # ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®æ±ºå®š
        overall_score = analysis_result.get('overall_score', 0)
        if overall_score < -0.5:
            risk_assessment['risk_level'] = 'high'
            risk_assessment['monitoring_points'].append('çŸ­æœŸçš„ãªæ¥­ç¸¾å‹•å‘ã®æ³¨æ„æ·±ã„ç›£è¦–ãŒå¿…è¦')
        elif overall_score < -0.2:
            risk_assessment['risk_level'] = 'medium'
            risk_assessment['monitoring_points'].append('ä¸­æœŸçš„ãªæ”¹å–„è¨ˆç”»ã®é€²æ—ç¢ºèªãŒé‡è¦')
        else:
            risk_assessment['risk_level'] = 'low'
        
        return risk_assessment
    
    def _generate_competitive_analysis(self, keywords: Dict, score: float) -> Dict[str, str]:
        """ç«¶äº‰ç’°å¢ƒåˆ†æ"""
        competitive_analysis = {
            'competitive_position': '',
            'market_strategy': '',
            'differentiation_factors': ''
        }
        
        positive_keywords = keywords.get('positive', [])
        
        # ç«¶äº‰åŠ›ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®åˆ†æ
        competitive_words = [kw for kw in positive_keywords if any(term in kw.get('word', '') for term in ['ç«¶äº‰åŠ›', 'å¼·åŒ–', 'ã‚·ã‚§ã‚¢', 'å¸‚å ´'])]
        
        if competitive_words:
            competitive_analysis['competitive_position'] = 'å¸‚å ´ã§ã®ç«¶äº‰å„ªä½æ€§ã‚’ç¢ºç«‹ã—ã€ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒã‚¸ã‚·ãƒ§ãƒ³ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚'
        elif score > 0.3:
            competitive_analysis['competitive_position'] = 'æ¥­ç•Œå†…ã§ã®åœ°ä½ã‚’ç€å®Ÿã«å‘ä¸Šã•ã›ã€ç«¶äº‰åŠ›ã‚’é«˜ã‚ã¦ã„ã¾ã™ã€‚'
        else:
            competitive_analysis['competitive_position'] = 'å®‰å®šã—ãŸäº‹æ¥­åŸºç›¤ã‚’ç¶­æŒã—ã€å …å®Ÿãªå¸‚å ´å‚åŠ è€…ã¨ã—ã¦ä½ç½®ã¥ã‘ã‚‰ã‚Œã¾ã™ã€‚'
        
        return competitive_analysis
    
    def _generate_future_outlook(self, analysis_result: Dict) -> Dict[str, str]:
        """å°†æ¥å±•æœ›"""
        future_outlook = {
            'short_term_outlook': '',
            'medium_term_strategy': '',
            'long_term_vision': ''
        }
        
        overall_score = analysis_result.get('overall_score', 0)
        sentiment_label = analysis_result.get('sentiment_label', 'neutral')
        
        if sentiment_label == 'positive':
            future_outlook['short_term_outlook'] = 'ä»Šå¾Œ1-2å¹´ã¯ç¶™ç¶šçš„ãªæˆé•·ãŒæœŸå¾…ã§ãã‚‹è¦‹é€šã—ã§ã™ã€‚'
            future_outlook['medium_term_strategy'] = 'ä¸­æœŸçš„ã«ã¯å¸‚å ´ã‚·ã‚§ã‚¢æ‹¡å¤§ã¨åç›Šæ€§å‘ä¸Šã®ä¸¡ç«‹ã‚’å›³ã‚‹æˆ¦ç•¥ãŒæœ‰åŠ¹ã§ã™ã€‚'
            future_outlook['long_term_vision'] = 'é•·æœŸçš„ã«ã¯æ¥­ç•Œã®ãƒªãƒ¼ãƒ€ãƒ¼ä¼æ¥­ã¨ã—ã¦ã®åœ°ä½ç¢ºç«‹ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚'
        elif sentiment_label == 'negative':
            future_outlook['short_term_outlook'] = 'çŸ­æœŸçš„ã«ã¯èª²é¡Œè§£æ±ºã¨æ§‹é€ æ”¹é©ã«æ³¨åŠ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚'
            future_outlook['medium_term_strategy'] = 'ä¸­æœŸçš„ãªå›å¾©è»Œé“ã¸ã®è»¢æ›ãŒé‡è¦ãªèª²é¡Œã¨ãªã‚Šã¾ã™ã€‚'
            future_outlook['long_term_vision'] = 'é•·æœŸçš„ã«ã¯æŒç¶šå¯èƒ½ãªãƒ“ã‚¸ãƒã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚'
        else:
            future_outlook['short_term_outlook'] = 'ç¾çŠ¶ç¶­æŒã‚’åŸºæœ¬ã¨ã—ã¦ã€ç€å®Ÿãªæˆé•·ã‚’ç›®æŒ‡ã™è¦‹é€šã—ã§ã™ã€‚'
            future_outlook['medium_term_strategy'] = 'å®‰å®šã—ãŸäº‹æ¥­åŸºç›¤ã®ä¸Šã«ã€é¸æŠçš„ãªæŠ•è³‡ã‚’è¡Œã†æˆ¦ç•¥ãŒé©åˆ‡ã§ã™ã€‚'
        
        return future_outlook
    
    def _generate_stakeholder_recommendations(self, score: float, label: str, statistics: Dict) -> Dict[str, List[str]]:
        """ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼åˆ¥æ¨å¥¨äº‹é …"""
        recommendations = {
            'for_investors': [],
            'for_management': [],
            'for_employees': [],
            'for_customers': []
        }
        
        if label == 'positive':
            recommendations['for_investors'] = [
                'æˆé•·æœŸå¾…ã«åŸºã¥ãæŠ•è³‡æˆ¦ç•¥ã®æ¤œè¨',
                'ä¸­é•·æœŸçš„ãªä¿æœ‰ã‚’å‰æã¨ã—ãŸæŠ•è³‡åˆ¤æ–­',
                'é…å½“æ”¿ç­–ã®å‹•å‘ã«æ³¨ç›®'
            ]
            recommendations['for_management'] = [
                'æˆé•·æˆ¦ç•¥ã®ç€å®Ÿãªå®Ÿè¡Œ',
                'ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¸ã®ç¶™ç¶šçš„ãªæƒ…å ±é–‹ç¤º',
                'æŒç¶šå¯èƒ½ãªæˆé•·åŸºç›¤ã®æ§‹ç¯‰'
            ]
        elif label == 'negative':
            recommendations['for_investors'] = [
                'ãƒªã‚¹ã‚¯è¦å› ã®è©³ç´°ãªåˆ†æã¨è©•ä¾¡',
                'æ”¹å–„è¨ˆç”»ã®é€²æ—çŠ¶æ³ã®å®šæœŸçš„ãªç¢ºèª',
                'åˆ†æ•£æŠ•è³‡ã«ã‚ˆã‚‹ãƒªã‚¹ã‚¯è»½æ¸›'
            ]
            recommendations['for_management'] = [
                'èª²é¡Œè§£æ±ºã¸ã®å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³ç­–å®š',
                'ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¨ã®ç©æ¥µçš„ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³',
                'æ§‹é€ æ”¹é©ã®åŠ é€ŸåŒ–'
            ]
        else:
            recommendations['for_investors'] = [
                'å®‰å®šé…å½“ã‚’é‡è¦–ã—ãŸãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªæ§‹ç¯‰',
                'æ¥­ç•Œå‹•å‘ã¨ã®æ¯”è¼ƒåˆ†æ',
                'é•·æœŸçš„ãªè¦–ç‚¹ã§ã®æŠ•è³‡åˆ¤æ–­'
            ]
        
        return recommendations


class TransparentSentimentAnalyzer:
    """åˆ†ã‹ã‚Šã‚„ã™ã„æ„Ÿæƒ…åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆè¦‹è§£ç”Ÿæˆå¼·åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, config: Optional[AnalysisConfig] = None):
        self.config = config or AnalysisConfig()
        self.dictionary = TransparentSentimentDictionary()
        self.text_processor = TransparentTextProcessor()
        self.insight_generator = UserInsightGenerator()
          # å•é¡Œã®æ ¹æœ¬åŸå› ã¨ä¿®æ­£ç®‡æ‰€

    def _analyze_keyword_frequency_safe(self, all_matches: List) -> Dict[str, List[Dict]]:
        """å®‰å…¨ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾é »åº¦ã®è©³ç´°åˆ†æï¼ˆä¸­ç«‹èªå½™ã‚‚å«ã‚€ç‰ˆï¼‰"""
        frequency_data = {'positive': [], 'negative': [], 'neutral': []}  # neutralã‚’è¿½åŠ 
        
        try:
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®æ¤œè¨¼
            if not all_matches:
                logger.warning("all_matchesãŒç©ºã§ã™")
                return frequency_data
            
            # æœ€åˆã®è¦ç´ ã®æ§‹é€ ã‚’ãƒã‚§ãƒƒã‚¯
            first_item = all_matches[0]
            logger.debug(f"æœ€åˆã®è¦ç´ : {first_item}, å‹: {type(first_item)}")
            
            # ã‚¿ãƒ—ãƒ«å½¢å¼ã‹ã©ã†ã‹ç¢ºèª
            if not isinstance(first_item, (tuple, list)) or len(first_item) != 3:
                logger.error(f"all_matchesã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ãŒä¸æ­£ã§ã™ã€‚æœŸå¾…: (word, score, type), å®Ÿéš›: {type(first_item)}")
                logger.error(f"all_matchesã‚µãƒ³ãƒ—ãƒ«: {all_matches[:5]}")
                return frequency_data
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾å›æ•°ã‚’é›†è¨ˆ
            keyword_counts = {}
            keyword_scores = {}
            keyword_types = {}
            
            for i, match_item in enumerate(all_matches):
                try:
                    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ç¢ºèª
                    if not isinstance(match_item, (tuple, list)) or len(match_item) != 3:
                        logger.warning(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹{i}ã®è¦ç´ ãŒä¸æ­£: {match_item}")
                        continue
                    
                    word, score, type_name = match_item
                    
                    # å‹ãƒã‚§ãƒƒã‚¯
                    if not isinstance(word, str):
                        logger.warning(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹{i}: wordãŒæ–‡å­—åˆ—ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {word} ({type(word)})")
                        continue
                    
                    if not isinstance(score, (int, float)):
                        logger.warning(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹{i}: scoreãŒæ•°å€¤ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {score} ({type(score)})")
                        continue
                    
                    if word not in keyword_counts:
                        keyword_counts[word] = 0
                        keyword_scores[word] = float(score)
                        keyword_types[word] = str(type_name)
                    
                    keyword_counts[word] += 1
                    # ã‚¹ã‚³ã‚¢ã¯å¹³å‡ã‚’å–ã‚‹
                    keyword_scores[word] = (keyword_scores[word] + float(score)) / 2
                    
                except Exception as e:
                    logger.error(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹{i}ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼: {e}, è¦ç´ : {match_item}")
                    continue
            
            logger.info(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é›†è¨ˆå®Œäº†: {len(keyword_counts)}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ¯ãƒ¼ãƒ‰")
            
            # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ»ä¸­ç«‹ã«åˆ†é¡
            for word, count in keyword_counts.items():
                try:
                    score = keyword_scores[word]
                    
                    keyword_data = {
                        'word': word,
                        'count': count,
                        'score': float(score),
                        'type': keyword_types[word],
                        'impact_level': self._get_impact_level(score),
                        'frequency_rank': 0  # å¾Œã§è¨­å®š
                    }
                    
                    # é–¾å€¤ã‚’ä½¿ã£ã¦åˆ†é¡ï¼ˆã‚ˆã‚Šå³å¯†ã«ï¼‰
                    if score > self.config.positive_threshold:  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.15
                        frequency_data['positive'].append(keyword_data)
                    elif score < self.config.negative_threshold:  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: -0.15
                        frequency_data['negative'].append(keyword_data)
                    else:
                        # ä¸­ç«‹çš„ãªèªå½™
                        frequency_data['neutral'].append(keyword_data)
                        
                except Exception as e:
                    logger.error(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰'{word}'ã®åˆ†é¡ã§ã‚¨ãƒ©ãƒ¼: {e}")
                    continue
            
            # å‡ºç¾å›æ•°ã§ã‚½ãƒ¼ãƒˆã—ã¦ãƒ©ãƒ³ã‚¯ä»˜ã‘
            for sentiment_type in ['positive', 'negative', 'neutral']:
                frequency_data[sentiment_type].sort(key=lambda x: x['count'], reverse=True)
                
                # ãƒ©ãƒ³ã‚¯ä»˜ã‘
                for i, item in enumerate(frequency_data[sentiment_type]):
                    item['frequency_rank'] = i + 1
            
            logger.info(f"é »åº¦åˆ†æå®Œäº†: ãƒã‚¸ãƒ†ã‚£ãƒ–{len(frequency_data['positive'])}èª, "
                    f"ãƒã‚¬ãƒ†ã‚£ãƒ–{len(frequency_data['negative'])}èª, "
                    f"ä¸­ç«‹{len(frequency_data['neutral'])}èª")
            
            return frequency_data
            
        except Exception as e:
            logger.error(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return frequency_data
        
    # âœ… ä¿®æ­£ç‰ˆã®ã‚³ãƒ¼ãƒ‰
    def analyze_text(self, text: str, session_id: str = None, document_info: Dict[str, str] = None) -> Dict[str, Any]:
        """é€æ˜æ€§ã®é«˜ã„æ„Ÿæƒ…åˆ†æï¼ˆãƒ‡ãƒ¼ã‚¿æ¸¡ã—å•é¡Œä¿®æ­£ç‰ˆï¼‰"""
        try:
            if not text or len(text.strip()) < 10:
                return self._empty_result(session_id)
            
            # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
            cleaned_text = self.text_processor.preprocess(text)
            
            # æ®µéšçš„ãªåˆ†æãƒ—ãƒ­ã‚»ã‚¹
            analysis_steps = []
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            context_matches = self._find_context_patterns(cleaned_text)
            if context_matches:
                analysis_steps.append({
                    'step': 'æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º',
                    'description': 'ã€Œæ¸›åã®æ”¹å–„ã€ã€Œæˆé•·ã®éˆåŒ–ã€ã®ã‚ˆã†ãªæ–‡è„ˆã‚’è€ƒæ…®ã—ãŸè¡¨ç¾ã‚’æ¤œå‡º',
                    'matches': context_matches,
                    'impact': sum(score for _, score, _ in context_matches)
                })
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: åŸºæœ¬èªå½™ã®æ¤œå‡º
            basic_matches = self._find_basic_words(cleaned_text, context_matches)
            if basic_matches:
                analysis_steps.append({
                    'step': 'åŸºæœ¬èªå½™æ¤œå‡º',
                    'description': 'æ„Ÿæƒ…è¾æ›¸ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹èªå½™ã‚’æ¤œå‡º',
                    'matches': basic_matches,
                    'impact': sum(score for _, score, _ in basic_matches)
                })
            
            # â˜…é‡è¦ï¼šå…¨ã¦ã®ãƒãƒƒãƒã‚’çµ±åˆï¼ˆã‚¿ãƒ—ãƒ«å½¢å¼ã‚’ç¶­æŒï¼‰
            all_matches = context_matches + basic_matches
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°: ãƒ‡ãƒ¼ã‚¿æ§‹é€ ç¢ºèª
            if all_matches:
                logger.info(f"all_matches ã‚µãƒ³ãƒ—ãƒ«: {all_matches[:3]}")
                logger.info(f"all_matches å‹: {type(all_matches)}, é•·ã•: {len(all_matches)}")
            
            # â˜…ä¿®æ­£ï¼šall_matchesï¼ˆã‚¿ãƒ—ãƒ«ãƒªã‚¹ãƒˆï¼‰ã‚’ãã®ã¾ã¾æ¸¡ã™
            score_calculation = self._calculate_detailed_score(all_matches)  # âœ… æ­£ã—ã„
            
            # å…¨ä½“ã‚¹ã‚³ã‚¢ã¨åˆ¤å®š
            overall_score = score_calculation['final_score']
            sentiment_label = self._determine_sentiment_label(overall_score)
            
            # åˆ†ææ ¹æ‹ ã®ç”Ÿæˆ
            analysis_reasoning = self._generate_reasoning(
                analysis_steps, score_calculation, overall_score, sentiment_label
            )
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æï¼ˆä¿®æ­£ç‰ˆï¼šall_matchesã‚’ä½¿ç”¨ï¼‰
            keyword_analysis = self._analyze_keywords(all_matches)
            
            # â˜…ä¿®æ­£ï¼šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦åˆ†æï¼ˆall_matchesã‚’ç›´æ¥ä½¿ç”¨ï¼‰
            keyword_frequency_data = self._analyze_keyword_frequency_safe(all_matches)  # âœ… æ­£ã—ã„
            
            # æ–‡ç« ãƒ¬ãƒ™ãƒ«åˆ†æ
            sentences = self._split_sentences(cleaned_text)
            sentence_analysis = self._analyze_sentences(sentences)
            
            # åŸºæœ¬çµæœã®æ§‹ç¯‰
            basic_result = {
                'overall_score': round(overall_score, 3),
                'sentiment_label': sentiment_label,
                'analysis_reasoning': analysis_reasoning,
                'score_calculation': score_calculation,
                'analysis_steps': analysis_steps,
                'keyword_analysis': keyword_analysis,
                'keyword_frequency_data': keyword_frequency_data,  # â˜…è¿½åŠ 
                'sample_sentences': {
                    'positive': [s for s in sentence_analysis if s['score'] > self.config.positive_threshold][:5],
                    'negative': [s for s in sentence_analysis if s['score'] < self.config.negative_threshold][:5],
                },
                'statistics': {
                    'total_words_analyzed': len(all_matches),
                    'context_patterns_found': len(context_matches),
                    'basic_words_found': len(basic_matches),
                    'sentences_analyzed': len(sentences),
                    'unique_words_found': len(set(word for word, _, _ in all_matches)),
                    'positive_words_count': len([s for _, s, _ in all_matches if s > 0]),
                    'negative_words_count': len([s for _, s, _ in all_matches if s < 0]),
                    'positive_sentences_count': len([s for s in sentence_analysis if s['score'] > self.config.positive_threshold]),
                    'negative_sentences_count': len([s for s in sentence_analysis if s['score'] < self.config.negative_threshold]),
                    'threshold_positive': self.config.positive_threshold,
                    'threshold_negative': self.config.negative_threshold,
                    # â˜…é »åº¦çµ±è¨ˆã‚’è¿½åŠ 
                    'total_keyword_occurrences': sum(item['count'] for item in keyword_frequency_data['positive'] + keyword_frequency_data['negative']),
                    'top_positive_keyword': keyword_frequency_data['positive'][0] if keyword_frequency_data['positive'] else None,
                    'top_negative_keyword': keyword_frequency_data['negative'][0] if keyword_frequency_data['negative'] else None,
                },
                'analysis_metadata': {
                    'analyzed_at': timezone.now().isoformat(),
                    'dictionary_size': len(self.dictionary.sentiment_dict),
                    'session_id': session_id,
                    'analysis_version': '2.3_data_flow_fixed',
                }
            }
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘è©³ç´°è¦‹è§£ã‚’ç”Ÿæˆ
            if document_info:
                user_insights = self.insight_generator.generate_detailed_insights(basic_result, document_info)
                basic_result['user_insights'] = user_insights
            
            return basic_result
            
        except Exception as e:
            logger.error(f"æ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            raise Exception(f"æ„Ÿæƒ…åˆ†æå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
    def _find_basic_words(self, text: str, context_matches: List) -> List[Tuple[str, float, str]]:
        """åŸºæœ¬èªå½™ã®æ¤œå‡ºï¼ˆèªå½™æƒ…å ±ä¿æŒç‰ˆï¼‰"""
        matches = []
        
        try:
            # æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œå‡ºã•ã‚ŒãŸèªå¥ã‚’é™¤å¤–å¯¾è±¡ã¨ã™ã‚‹
            context_words = {word for word, _, _ in context_matches}
            
            # è¾æ›¸ã®ã™ã¹ã¦ã®èªå½™ã‚’ãƒã‚§ãƒƒã‚¯
            for word, score in self.dictionary.sentiment_dict.items():
                if len(word) < 1:
                    continue
                    
                if word in context_words:
                    continue
                
                # ãƒ†ã‚­ã‚¹ãƒˆå†…ã§ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                count = text.count(word)
                if count > 0:
                    # å‡ºç¾å›æ•°åˆ†ã ã‘è¿½åŠ ï¼ˆæœ€å¤§5å›ã¾ã§ï¼‰
                    for _ in range(min(count, 5)):
                        matches.append((word, score, 'åŸºæœ¬èªå½™'))  # â˜…é‡è¦: ã‚¿ãƒ—ãƒ«å½¢å¼
            
            return matches
            
        except Exception as e:
            logger.debug(f"åŸºæœ¬èªå½™æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
        
    def _find_context_patterns(self, text: str) -> List[Tuple[str, float, str]]:
        """æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºï¼ˆèªå½™æƒ…å ±ä¿æŒç‰ˆï¼‰"""
        matches = []
        
        try:
            # æ”¹å–„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            for pattern in self.dictionary.improvement_patterns:
                for match in re.finditer(pattern, text, re.IGNORECASE):
                    matched_text = match.group()
                    score = 0.7
                    matches.append((matched_text, score, 'æ”¹å–„è¡¨ç¾'))  # â˜…é‡è¦: ã‚¿ãƒ—ãƒ«å½¢å¼
            
            # æ‚ªåŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            for pattern in self.dictionary.deterioration_patterns:
                for match in re.finditer(pattern, text, re.IGNORECASE):
                    matched_text = match.group()
                    score = -0.6
                    matches.append((matched_text, score, 'æ‚ªåŒ–è¡¨ç¾'))  # â˜…é‡è¦: ã‚¿ãƒ—ãƒ«å½¢å¼
            
            # å¦å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            for pattern in self.dictionary.negation_patterns:
                for match in re.finditer(pattern, text, re.IGNORECASE):
                    matched_text = match.group()
                    score = 0.4
                    matches.append((matched_text, score, 'å¦å®šè¡¨ç¾'))  # â˜…é‡è¦: ã‚¿ãƒ—ãƒ«å½¢å¼
            
            return matches
            
        except Exception as e:
            logger.debug(f"æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []
               
    def _calculate_detailed_score(self, all_matches: List[Tuple[str, float, str]]) -> Dict:
        """è©³ç´°ãªã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆæ–¹æ³•1ï¼šé‡è¤‡é‡ã¿ä»˜ã‘æ–¹å¼ï¼‰"""
        if not all_matches:
            return {
                'raw_scores': [], 'positive_scores': [], 'negative_scores': [],
                'positive_words': [], 'negative_words': [],
                'positive_sum': 0.0, 'negative_sum': 0.0, 'score_count': 0,
                'average_score': 0.0, 'final_score': 0.0,
            }
        
        # ãƒ‡ãƒãƒƒã‚°: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        logger.info(f"_calculate_detailed_score å…¥åŠ›: {len(all_matches)}å€‹ã®ãƒãƒƒãƒ")
        logger.info(f"ã‚µãƒ³ãƒ—ãƒ«ãƒãƒƒãƒ: {all_matches[:3] if all_matches else 'ç„¡ã—'}")
        
        # === ğŸ”¥ é‡è¤‡é‡ã¿ä»˜ã‘æ–¹å¼ã®å®Ÿè£… ===
        
        # 1. èªå½™åˆ¥ã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        word_frequency = {}
        for word, score, type_name in all_matches:
            key = f"{word}_{type_name}"
            if key not in word_frequency:
                word_frequency[key] = {
                    'word': word,
                    'score': score,
                    'type': type_name,
                    'count': 0
                }
            word_frequency[key]['count'] += 1
        
        # 2. é‡è¤‡é‡ã¿ä»˜ã‘è¨ˆç®—
        def calculate_repetition_weight(count: int) -> float:
            """é‡è¤‡å›æ•°ã«å¿œã˜ãŸé‡ã¿è¨ˆç®—"""
            if count == 1:
                return 1.0
            elif count == 2:
                return 1.8  # 1.0 + 0.8
            elif count == 3:
                return 2.4  # 1.0 + 0.8 + 0.6
            else:
                # 4å›ç›®ä»¥é™ã¯0.4ãšã¤å¢—åŠ 
                return 2.4 + (count - 3) * 0.4
        
        # 3. é‡ã¿ä»˜ã‘ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        weighted_results = []
        positive_weighted_sum = 0.0
        negative_weighted_sum = 0.0
        
        for word_data in word_frequency.values():
            base_score = word_data['score']
            count = word_data['count']
            
            # é‡è¤‡é‡ã¿ä»˜ã‘ã®é©ç”¨
            repetition_weight = calculate_repetition_weight(count)
            weighted_score = base_score * repetition_weight
            
            weighted_results.append({
                'word': word_data['word'],
                'score': base_score,
                'type': word_data['type'],
                'count': count,
                'repetition_weight': repetition_weight,
                'weighted_score': weighted_score,
                'total_contribution': weighted_score,
                'impact_level': self._get_impact_level(abs(weighted_score))
            })
            
            # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ã®åˆè¨ˆ
            if weighted_score > 0:
                positive_weighted_sum += weighted_score
            elif weighted_score < 0:
                negative_weighted_sum += weighted_score
        
        # 4. æœ€çµ‚ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        total_weighted_sum = positive_weighted_sum + negative_weighted_sum
        unique_words_count = len(word_frequency)
        
        # é‡è¦ï¼šãƒ¦ãƒ‹ãƒ¼ã‚¯èªå½™æ•°ã§å‰²ã‚‹ï¼ˆé‡è¤‡ã®æ„å‘³ã‚’ä¿æŒï¼‰
        final_score = total_weighted_sum / unique_words_count if unique_words_count > 0 else 0.0
        
        # 5. æ­£è¦åŒ–ï¼ˆ-1.0ã€œ1.0ã®ç¯„å›²ã«åˆ¶é™ï¼‰
        final_score = max(-1.0, min(1.0, final_score))
        
        # 6. è¡¨ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        positive_words = [w for w in weighted_results if w['score'] > 0]
        negative_words = [w for w in weighted_results if w['score'] < 0]
        
        # ç·è²¢çŒ®åº¦ã§ã‚½ãƒ¼ãƒˆ
        positive_words.sort(key=lambda x: x['total_contribution'], reverse=True)
        negative_words.sort(key=lambda x: x['total_contribution'])
        
        # 7. çµ±è¨ˆæƒ…å ±ã®è¨ˆç®—
        all_raw_scores = [score for _, score, _ in all_matches]
        positive_raw_scores = [score for score in all_raw_scores if score > 0]
        negative_raw_scores = [score for score in all_raw_scores if score < 0]
        
        # 8. ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®å‡ºåŠ›
        logger.info("=== é‡è¤‡é‡ã¿ä»˜ã‘æ–¹å¼ã®è¨ˆç®—çµæœ ===")
        logger.info(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯èªå½™æ•°: {unique_words_count}")
        logger.info(f"ç·èªå½™å‡ºç¾æ•°: {len(all_matches)}")
        logger.info(f"ãƒã‚¸ãƒ†ã‚£ãƒ–é‡ã¿ä»˜ã‘åˆè¨ˆ: {positive_weighted_sum:.3f}")
        logger.info(f"ãƒã‚¬ãƒ†ã‚£ãƒ–é‡ã¿ä»˜ã‘åˆè¨ˆ: {negative_weighted_sum:.3f}")
        logger.info(f"æœ€çµ‚ã‚¹ã‚³ã‚¢: {final_score:.3f}")
        
        # é‡è¤‡åŠ¹æœã®è©³ç´°ãƒ­ã‚°
        for word_data in weighted_results:
            if word_data['count'] > 1:
                logger.info(f"  {word_data['word']}: {word_data['score']:.2f} Ã— {word_data['count']}å› Ã— é‡ã¿{word_data['repetition_weight']:.1f} = {word_data['weighted_score']:.3f}")
        
        logger.info("=====================================")
        
        return {
            # åŸºæœ¬çµ±è¨ˆ
            'raw_scores': all_raw_scores,
            'positive_scores': positive_raw_scores,
            'negative_scores': negative_raw_scores,
            'positive_sum': positive_weighted_sum,
            'negative_sum': negative_weighted_sum,
            'score_count': len(all_raw_scores),
            'average_score': sum(all_raw_scores) / len(all_raw_scores) if all_raw_scores else 0,
            'final_score': final_score,
            
            # é‡è¤‡é‡ã¿ä»˜ã‘ç‰¹æœ‰ã®æƒ…å ±
            'positive_words': positive_words,
            'negative_words': negative_words,
            'unique_words_count': unique_words_count,
            'total_occurrences': len(all_matches),
            'repetition_factor': len(all_matches) / unique_words_count if unique_words_count > 0 else 1.0,
            
            # é‡ã¿ä»˜ã‘è©³ç´°æƒ…å ±
            'weighted_positive_sum': positive_weighted_sum,
            'weighted_negative_sum': negative_weighted_sum,
            'weighted_total_sum': total_weighted_sum,
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            'calculation_method': 'repetition_weighted',
            'calculation_explanation': f'é‡è¤‡é‡ã¿ä»˜ã‘æ–¹å¼ï¼š{unique_words_count}å€‹ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯èªå½™ã‚’å‡ºç¾å›æ•°ã«å¿œã˜ã¦é‡ã¿ä»˜ã‘',
            'weight_formula': '1å›:1.0å€, 2å›:1.8å€, 3å›:2.4å€, 4å›ä»¥é™:+0.4å€ãšã¤'
        }

        
    def _generate_reasoning(self, analysis_steps: List, score_calc: Dict, overall_score: float, sentiment_label: str) -> Dict:
        """åˆ†ææ ¹æ‹ ã®ç”Ÿæˆ"""
        reasoning = {
            'summary': '',
            'key_factors': [],
            'score_breakdown': '',
            'conclusion': ''
        }
        
        # ä¸»è¦å› å­ã®ç‰¹å®š
        pos_count = len(score_calc['positive_scores'])
        neg_count = len(score_calc['negative_scores'])
        
        if pos_count > neg_count:
            reasoning['key_factors'].append(f'ãƒã‚¸ãƒ†ã‚£ãƒ–ãªè¡¨ç¾ãŒ{pos_count}å€‹æ¤œå‡ºã•ã‚Œã¾ã—ãŸ')
        elif neg_count > pos_count:
            reasoning['key_factors'].append(f'ãƒã‚¬ãƒ†ã‚£ãƒ–ãªè¡¨ç¾ãŒ{neg_count}å€‹æ¤œå‡ºã•ã‚Œã¾ã—ãŸ')
        else:
            reasoning['key_factors'].append('ãƒã‚¸ãƒ†ã‚£ãƒ–ã¨ãƒã‚¬ãƒ†ã‚£ãƒ–ãªè¡¨ç¾ãŒåŒæ•°æ¤œå‡ºã•ã‚Œã¾ã—ãŸ')
        
        # æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®å½±éŸ¿
        context_steps = [step for step in analysis_steps if 'æ–‡è„ˆ' in step['step']]
        if context_steps:
            context_impact = context_steps[0]['impact']
            if context_impact > 0:
                reasoning['key_factors'].append('ã€Œæ¸›åã®æ”¹å–„ã€ã®ã‚ˆã†ãªæ–‡è„ˆã‚’è€ƒæ…®ã—ãŸæ”¹å–„è¡¨ç¾ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ')
            elif context_impact < 0:
                reasoning['key_factors'].append('ã€Œæˆé•·ã®éˆåŒ–ã€ã®ã‚ˆã†ãªæ–‡è„ˆã‚’è€ƒæ…®ã—ãŸæ‚ªåŒ–è¡¨ç¾ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ')
        
        # ã‚¹ã‚³ã‚¢ã®å†…è¨³èª¬æ˜
        if score_calc['positive_sum'] and score_calc['negative_sum']:
            reasoning['score_breakdown'] = (
                f'ãƒã‚¸ãƒ†ã‚£ãƒ–åˆè¨ˆ: {score_calc["positive_sum"]:.2f}, '
                f'ãƒã‚¬ãƒ†ã‚£ãƒ–åˆè¨ˆ: {score_calc["negative_sum"]:.2f}, '
                f'å¹³å‡ã‚¹ã‚³ã‚¢: {score_calc["average_score"]:.2f}'
            )
        elif score_calc['positive_sum']:
            reasoning['score_breakdown'] = f'ãƒã‚¸ãƒ†ã‚£ãƒ–è¡¨ç¾ã®ã¿æ¤œå‡º: åˆè¨ˆ{score_calc["positive_sum"]:.2f}'
        elif score_calc['negative_sum']:
            reasoning['score_breakdown'] = f'ãƒã‚¬ãƒ†ã‚£ãƒ–è¡¨ç¾ã®ã¿æ¤œå‡º: åˆè¨ˆ{score_calc["negative_sum"]:.2f}'
        else:
            reasoning['score_breakdown'] = 'æ„Ÿæƒ…ã‚’è¡¨ã™è¡¨ç¾ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ'
        
        # çµè«–
        if sentiment_label == 'positive':
            if overall_score > 0.6:
                reasoning['conclusion'] = 'éå¸¸ã«ãƒã‚¸ãƒ†ã‚£ãƒ–ãªå†…å®¹ã§ã™'
            else:
                reasoning['conclusion'] = 'ã‚„ã‚„å‰å‘ããªå†…å®¹ã§ã™'
        elif sentiment_label == 'negative':
            if overall_score < -0.6:
                reasoning['conclusion'] = 'éå¸¸ã«ãƒã‚¬ãƒ†ã‚£ãƒ–ãªå†…å®¹ã§ã™'
            else:
                reasoning['conclusion'] = 'ã‚„ã‚„æ…é‡ãªå†…å®¹ã§ã™'
        else:
            reasoning['conclusion'] = 'ä¸­ç«‹çš„ãªå†…å®¹ã§ã™'
        
        # è¦ç´„
        reasoning['summary'] = f'{reasoning["conclusion"]}ã€‚{reasoning["key_factors"][0] if reasoning["key_factors"] else ""}'
        
        return reasoning
    
    def _analyze_keywords(self, matches: List[Tuple[str, float, str]]) -> Dict:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æï¼ˆåˆ†ã‹ã‚Šã‚„ã™ã„å½¢å¼ï¼‰"""
        positive_words = []
        negative_words = []
        
        for word, score, type_name in matches:
            word_info = {
                'word': word,
                'score': round(score, 2),
                'type': type_name,
                'impact': 'å¼·ã„' if abs(score) > 0.7 else 'ä¸­ç¨‹åº¦' if abs(score) > 0.4 else 'è»½å¾®'
            }
            
            if score > 0:
                positive_words.append(word_info)
            elif score < 0:
                negative_words.append(word_info)
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        positive_words.sort(key=lambda x: x['score'], reverse=True)
        negative_words.sort(key=lambda x: x['score'])
        
        return {
            'positive': positive_words[:10],  # ä¸Šä½10ä»¶
            'negative': negative_words[:10],  # ä¸Šä½10ä»¶
        }
    
    def _split_sentences(self, text: str) -> List[str]:
        """æ–‡åˆ†å‰²ï¼ˆã‚ˆã‚ŠçŸ­ã„æ–‡ç« ã‚‚å¯¾è±¡ï¼‰"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
        return [s.strip() for s in sentences if len(s.strip()) >= self.config.min_sentence_length and 
                len(re.findall(r'[ã-ã‚“ã‚¡-ãƒ¶ä¸€-é¾¯]', s)) > 2]  # æ—¥æœ¬èªæ–‡å­—ãŒ2å€‹ä»¥ä¸Š
        
    def _analyze_sentences(self, sentences: List[str]) -> List[Dict]:
        """æ–‡ç« ãƒ¬ãƒ™ãƒ«åˆ†æï¼ˆé‡è¤‡é™¤å»å¼·åŒ–ç‰ˆï¼‰"""
        sentence_analysis = []
        analyzed_texts = set()  # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
        
        for sentence in sentences[:self.config.max_sample_sentences]:
            # ç°¡æ˜“çš„ãªæ–‡ã‚¹ã‚³ã‚¢è¨ˆç®—
            context_matches = self._find_context_patterns(sentence)
            basic_matches = self._find_basic_words(sentence, context_matches)
            
            all_scores = [score for _, score, _ in context_matches + basic_matches]
            sent_score = sum(all_scores) / len(all_scores) if all_scores else 0
            
            if abs(sent_score) > 0.15:  # é–¾å€¤ã‚’0.15ã«ä¸‹ã’ã¦æ–‡ç« ã‚’å–å¾—ã—ã‚„ã™ãã™ã‚‹
                # æ–‡ç« ã®æ­£è¦åŒ–ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
                normalized_text = self._normalize_sentence_for_dedup(sentence)
                
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if normalized_text in analyzed_texts:
                    continue
                    
                analyzed_texts.add(normalized_text)
                
                keywords = [word for word, _, _ in context_matches + basic_matches]
                # ä¸€åº¦ã«ã™ã¹ã¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
                highlighted_text = self._highlight_all_keywords_in_text(sentence, keywords)
                
                sentence_analysis.append({
                    'text': sentence[:200],  # æ–‡å­—æ•°åˆ¶é™
                    'highlighted_text': highlighted_text,
                    'score': round(sent_score, 2),
                    'keywords': list(set(keywords)),  # é‡è¤‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é™¤å»
                })
        
        return sentence_analysis

    def _normalize_sentence_for_dedup(self, sentence: str) -> str:
        """é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®æ–‡ç« æ­£è¦åŒ–"""
        import re
        
        # ç©ºç™½ã‚„è¨˜å·ã‚’çµ±ä¸€
        normalized = re.sub(r'\s+', ' ', sentence)
        normalized = re.sub(r'[ã€‚ã€ï¼ï¼Ÿ\.,!?]', '', normalized)
        normalized = normalized.strip().lower()
        
        # 50æ–‡å­—ä»¥ä¸Šã®å ´åˆã¯æœ€åˆã®50æ–‡å­—ã§é‡è¤‡åˆ¤å®š
        if len(normalized) > 50:
            normalized = normalized[:50]
        
        return normalized

    def _highlight_all_keywords_in_text(self, text: str, keywords: List[str]) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ã™ã¹ã¦ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ä¸€åº¦ã«ãƒã‚¤ãƒ©ã‚¤ãƒˆ"""
        highlighted_text = text[:200]  # æ–‡å­—æ•°åˆ¶é™
        
        if not keywords:
            return highlighted_text
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é•·ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ã€éƒ¨åˆ†ãƒãƒƒãƒã«ã‚ˆã‚‹é‡è¤‡ã‚’é¿ã‘ã‚‹
        sorted_keywords = sorted(set(keywords), key=len, reverse=True)
        
        for keyword in sorted_keywords:
            if keyword and keyword in highlighted_text:
                # æ—¢ã«ãƒã‚¤ãƒ©ã‚¤ãƒˆã•ã‚Œã¦ã„ã‚‹éƒ¨åˆ†ã¯é™¤å¤–
                if f'<span class="keyword-highlight">{keyword}</span>' not in highlighted_text:
                    highlighted_text = highlighted_text.replace(
                        keyword,
                        f'<span class="keyword-highlight">{keyword}</span>'
                    )
        
        return highlighted_text
    
    def _highlight_keywords_in_text(self, text: str, keywords: List[str]) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆå†…ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ"""
        highlighted_text = text[:200]  # æ–‡å­—æ•°åˆ¶é™
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é•·ã„é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ã€éƒ¨åˆ†ãƒãƒƒãƒã«ã‚ˆã‚‹é‡è¤‡ã‚’é¿ã‘ã‚‹
        sorted_keywords = sorted(set(keywords), key=len, reverse=True)
        
        for keyword in sorted_keywords:
            if keyword and keyword in highlighted_text:
                # HTMLã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚Œã¦ã„ãªã„çŠ¶æ…‹ã§ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚¿ã‚°ã‚’æŒ¿å…¥
                highlighted_text = highlighted_text.replace(
                    keyword,
                    f'<span class="keyword-highlight">{keyword}</span>'
                )
        
        return highlighted_text
    
    def _determine_sentiment_label(self, score: float) -> str:
        """æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«æ±ºå®š"""
        if score > self.config.positive_threshold:
            return 'positive'
        elif score < self.config.negative_threshold:
            return 'negative'
        else:
            return 'neutral'
    
    def _empty_result(self, session_id: str = None) -> Dict[str, Any]:
        """ç©ºçµæœã®ç”Ÿæˆ"""
        return {
            'overall_score': 0.0,
            'sentiment_label': 'neutral',
            'analysis_reasoning': {
                'summary': 'æ„Ÿæƒ…ã‚’è¡¨ã™è¡¨ç¾ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ',
                'key_factors': [],
                'score_breakdown': 'åˆ†æå¯¾è±¡ã¨ãªã‚‹èªå½™ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ',
                'conclusion': 'ä¸­ç«‹çš„ãªå†…å®¹ã§ã™'
            },
            'score_calculation': {
                'raw_scores': [], 'positive_scores': [], 'negative_scores': [],
                'positive_sum': 0.0, 'negative_sum': 0.0, 'score_count': 0,
                'average_score': 0.0, 'final_score': 0.0,
            },
            'analysis_steps': [],
            'keyword_analysis': {'positive': [], 'negative': []},
            'sample_sentences': {'positive': [], 'negative': []},
            'statistics': {
                'total_words_analyzed': 0, 'context_patterns_found': 0,
                'basic_words_found': 0, 'sentences_analyzed': 0, 'unique_words_found': 0,
                'positive_words_count': 0, 'negative_words_count': 0,
                'positive_sentences_count': 0, 'negative_sentences_count': 0,
                'threshold_positive': self.config.positive_threshold,
                'threshold_negative': self.config.negative_threshold,
            },
            'analysis_metadata': {
                'analyzed_at': timezone.now().isoformat(),
                'dictionary_size': len(self.dictionary.sentiment_dict),
                'session_id': session_id,
                'analysis_version': '2.1_insight_enhanced',
            }
        }
    
    def _integrate_keywords(self, keyword_list: List[Dict]) -> List[Dict]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®çµ±åˆï¼ˆé‡è¤‡é™¤å»ãƒ»ã‚¹ã‚³ã‚¢é›†ç´„ï¼‰"""
        keyword_map = {}
        
        for keyword_info in keyword_list:
            word = keyword_info.get('word', '')
            score = keyword_info.get('score', 0)
            type_name = keyword_info.get('type', '')
            impact = keyword_info.get('impact', '')
            section = keyword_info.get('section', '')
            
            if word in keyword_map:
                # æ—¢å­˜ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã€ã‚¹ã‚³ã‚¢ã‚’å¹³å‡åŒ–ã—ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³æƒ…å ±ã‚’è¿½åŠ 
                existing = keyword_map[word]
                existing['score'] = (existing['score'] + score) / 2
                existing['sections'] = existing.get('sections', []) + [section]
                existing['occurrences'] = existing.get('occurrences', 1) + 1
                
                # ã‚ˆã‚Šå¼·ã„å½±éŸ¿åº¦ã‚’æ¡ç”¨
                impact_priority = {'å¼·ã„': 3, 'ä¸­ç¨‹åº¦': 2, 'è»½å¾®': 1}
                if impact_priority.get(impact, 0) > impact_priority.get(existing['impact'], 0):
                    existing['impact'] = impact
            else:
                # æ–°ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å ´åˆ
                keyword_map[word] = {
                    'word': word,
                    'score': score,
                    'type': type_name,
                    'impact': impact,
                    'sections': [section],
                    'occurrences': 1
                }
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        integrated_keywords = list(keyword_map.values())
        integrated_keywords.sort(key=lambda x: abs(x['score']), reverse=True)
        
        return integrated_keywords

    def analyze_text_sections(self, text_sections: Dict[str, str], session_id: str = None, document_info: Dict[str, str] = None) -> Dict[str, Any]:
        """è¤‡æ•°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®åˆ†æï¼ˆçµ±è¨ˆä¿®æ­£ç‰ˆï¼‰"""
        try:
            section_results = {}
            all_positive_sentences = []
            all_negative_sentences = []
            all_positive_keywords = []
            all_negative_keywords = []
            combined_steps = []
            
            # â˜…ä¿®æ­£: çµ±è¨ˆè¨ˆç®—ç”¨ã®å¤‰æ•°ã‚’è¿½åŠ 
            total_context_patterns = 0
            total_sentences_analyzed = 0
            
            # â˜…ä¿®æ­£ï¼šçµ±åˆç”¨ã®ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ï¼ˆã‚¿ãƒ—ãƒ«å½¢å¼ï¼‰
            all_matches_combined = []
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®ã‚»ãƒƒãƒˆ
            seen_positive_sentences = set()
            seen_negative_sentences = set()
            
            # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥åˆ†æ
            for section_name, text in text_sections.items():
                if len(text.strip()) < 50:
                    continue
                
                result = self.analyze_text(text, session_id)
                section_results[section_name] = result
                combined_steps.extend(result['analysis_steps'])
                
                # â˜…ä¿®æ­£: å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµ±è¨ˆã‚’ç´¯ç©
                section_stats = result.get('statistics', {})
                total_context_patterns += section_stats.get('context_patterns_found', 0)
                total_sentences_analyzed += section_stats.get('sentences_analyzed', 0)
                
                # â˜…ä¿®æ­£ï¼šanalysis_stepsã‹ã‚‰ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’æ­£ã—ãå–å¾—
                for step in result.get('analysis_steps', []):
                    matches = step.get('matches', [])
                    if matches:
                        for match in matches:
                            if isinstance(match, (tuple, list)) and len(match) == 3:
                                all_matches_combined.append(match)
                            else:
                                logger.warning(f"ä¸æ­£ãªãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {match}")
                
                # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®çµæœã‚’çµ±åˆãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆé‡è¤‡é™¤å»ï¼‰
                sample_sentences = result.get('sample_sentences', {})
                keyword_analysis = result.get('keyword_analysis', {})
                
                # ãƒã‚¸ãƒ†ã‚£ãƒ–æ–‡ç« ã®çµ±åˆï¼ˆé‡è¤‡é™¤å»ï¼‰
                positive_sentences = sample_sentences.get('positive', [])
                for sentence in positive_sentences:
                    normalized = self._normalize_sentence_for_dedup(sentence.get('text', ''))
                    if normalized not in seen_positive_sentences:
                        sentence['section'] = section_name
                        all_positive_sentences.append(sentence)
                        seen_positive_sentences.add(normalized)
                
                # ãƒã‚¬ãƒ†ã‚£ãƒ–æ–‡ç« ã®çµ±åˆï¼ˆé‡è¤‡é™¤å»ï¼‰
                negative_sentences = sample_sentences.get('negative', [])
                for sentence in negative_sentences:
                    normalized = self._normalize_sentence_for_dedup(sentence.get('text', ''))
                    if normalized not in seen_negative_sentences:
                        sentence['section'] = section_name
                        all_negative_sentences.append(sentence)
                        seen_negative_sentences.add(normalized)
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®çµ±åˆ
                positive_keywords = keyword_analysis.get('positive', [])
                negative_keywords = keyword_analysis.get('negative', [])
                
                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³åã‚’è¿½åŠ ã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’çµ±åˆ
                for keyword in positive_keywords:
                    keyword['section'] = section_name
                    all_positive_keywords.append(keyword)
                
                for keyword in negative_keywords:
                    keyword['section'] = section_name
                    all_negative_keywords.append(keyword)
            
            if not all_matches_combined:
                return self._empty_result(session_id)
            
            # çµ±åˆåˆ†æ
            combined_score_calc = self._calculate_detailed_score(all_matches_combined)
            overall_score = combined_score_calc['final_score']
            sentiment_label = self._determine_sentiment_label(overall_score)
            
            # çµ±åˆåˆ†ææ ¹æ‹ 
            integrated_reasoning = self._generate_reasoning(
                combined_steps, combined_score_calc, overall_score, sentiment_label
            )
            
            # â˜…ä¿®æ­£ï¼šçµ±åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦åˆ†æ
            integrated_keyword_frequency = self._analyze_keyword_frequency_safe(all_matches_combined)
            
            
            # â˜…ä¿®æ­£ï¼šçµ±åˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦åˆ†æï¼ˆall_matches_combinedã‚’ä½¿ç”¨ï¼‰
            logger.debug(f"çµ±åˆãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ç¢ºèª: {len(all_matches_combined)}ä»¶")
            if all_matches_combined:
                logger.debug(f"çµ±åˆãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«: {all_matches_combined[:3]}")
            
            integrated_keyword_frequency = self._analyze_keyword_frequency_safe(all_matches_combined)
            
            # çµ±åˆã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ–‡ç« ï¼ˆã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆï¼‰
            all_positive_sentences.sort(key=lambda x: x['score'], reverse=True)
            all_negative_sentences.sort(key=lambda x: x['score'])
            
            # çµ±åˆã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æï¼ˆé‡è¤‡é™¤å»ã¨ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆï¼‰
            integrated_positive_keywords = self._integrate_keywords(all_positive_keywords)
            integrated_negative_keywords = self._integrate_keywords(all_negative_keywords)
            
            # â˜…ä¿®æ­£: æ­£ã—ã„çµ±è¨ˆæƒ…å ±ã®æ§‹ç¯‰
            basic_result = {
                'overall_score': round(overall_score, 3),
                'sentiment_label': sentiment_label,
                'analysis_reasoning': integrated_reasoning,
                'score_calculation': combined_score_calc,
                'section_analysis': section_results,
                'keyword_frequency_data': integrated_keyword_frequency,
                'sample_sentences': {
                    'positive': all_positive_sentences[:10],
                    'negative': all_negative_sentences[:10],
                },
                'keyword_analysis': {
                    'positive': integrated_positive_keywords[:15],
                    'negative': integrated_negative_keywords[:15],
                },
                'statistics': {
                    'sections_analyzed': len(section_results),
                    'total_words_analyzed': len(all_matches_combined),
                    # â˜…ä¿®æ­£: æ­£ã—ã„æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³æ•°
                    'context_patterns_found': total_context_patterns,
                    # â˜…ä¿®æ­£: æ­£ã—ã„åˆ†ææ–‡æ•°  
                    'sentences_analyzed': total_sentences_analyzed,
                    'basic_words_found': sum(r['statistics'].get('basic_words_found', 0) for r in section_results.values()),
                    'unique_words_found': len(set(word for word, _, _ in all_matches_combined)),
                    'positive_sentences_found': len(all_positive_sentences),
                    'negative_sentences_found': len(all_negative_sentences),
                    'total_positive_keywords': len(all_positive_keywords),
                    'total_negative_keywords': len(all_negative_keywords),
                    'positive_words_count': len([s for _, s, _ in all_matches_combined if s > 0]),
                    'negative_words_count': len([s for _, s, _ in all_matches_combined if s < 0]),
                    'positive_sentences_count': len(all_positive_sentences),
                    'negative_sentences_count': len(all_negative_sentences),
                    'threshold_positive': self.config.positive_threshold,
                    'threshold_negative': self.config.negative_threshold,
                    # é »åº¦çµ±è¨ˆ
                    'total_keyword_occurrences': sum(item['count'] for item in integrated_keyword_frequency['positive'] + integrated_keyword_frequency['negative']),
                    'top_positive_keyword': integrated_keyword_frequency['positive'][0] if integrated_keyword_frequency['positive'] else None,
                    'top_negative_keyword': integrated_keyword_frequency['negative'][0] if integrated_keyword_frequency['negative'] else None,
                },
                'analysis_metadata': {
                    'analyzed_at': timezone.now().isoformat(),
                    'dictionary_size': len(self.dictionary.sentiment_dict),
                    'session_id': session_id,
                    'sections_analyzed': list(text_sections.keys()),
                    'analysis_version': '2.4_stats_fixed',
                    'integration_method': 'section_aggregation_with_proper_stats',
                }
            }
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘è©³ç´°è¦‹è§£ã‚’ç”Ÿæˆ
            if document_info:
                user_insights = self.insight_generator.generate_detailed_insights(basic_result, document_info)
                basic_result['user_insights'] = user_insights
            
            # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°
            logger.info(f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³çµ±åˆåˆ†æå®Œäº†ï¼ˆçµ±è¨ˆä¿®æ­£ç‰ˆï¼‰: {len(section_results)}ã‚»ã‚¯ã‚·ãƒ§ãƒ³, "
                    f"æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³{total_context_patterns}å€‹, "
                    f"åˆ†ææ–‡æ•°{total_sentences_analyzed}æ–‡, "
                    f"ãƒã‚¸ãƒ†ã‚£ãƒ–æ–‡ç« {len(all_positive_sentences)}ä»¶, "
                    f"ãƒã‚¬ãƒ†ã‚£ãƒ–æ–‡ç« {len(all_negative_sentences)}ä»¶")
            
            return basic_result
            
        except Exception as e:
            logger.error(f"ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            raise Exception(f"æ„Ÿæƒ…åˆ†æå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

    # å½±éŸ¿ãƒ¬ãƒ™ãƒ«åˆ¤å®šã‚’é‡ã¿ä»˜ã‘ã‚¹ã‚³ã‚¢ã«å¯¾å¿œ
    def _get_impact_level(self, weighted_score_abs: float) -> str:
        """é‡ã¿ä»˜ã‘ã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹å½±éŸ¿ãƒ¬ãƒ™ãƒ«åˆ¤å®š"""
        if weighted_score_abs >= 2.0:
            return 'very_high'
        elif weighted_score_abs >= 1.5:
            return 'high'
        elif weighted_score_abs >= 1.0:
            return 'medium'
        elif weighted_score_abs >= 0.5:
            return 'low'
        else:
            return 'very_low'

        
class SentimentAnalysisService:
    """æ„Ÿæƒ…åˆ†æã‚µãƒ¼ãƒ“ã‚¹ï¼ˆè¦‹è§£ç”Ÿæˆå¼·åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        self.analyzer = TransparentSentimentAnalyzer()
        self.xbrl_service = EDINETXBRLService()
    
    def start_analysis(self, document_id: str, force: bool = False, user_ip: str = None) -> Dict[str, Any]:
        """æ„Ÿæƒ…åˆ†æé–‹å§‹"""
        from ..models import DocumentMetadata, SentimentAnalysisSession
        
        try:
            document = DocumentMetadata.objects.get(doc_id=document_id, legal_status='1')
            
            if not force:
                recent_session = SentimentAnalysisSession.objects.filter(
                    document=document,
                    processing_status='COMPLETED',
                    created_at__gte=timezone.now() - timedelta(hours=1)
                ).first()
                
                if recent_session:
                    return {
                        'status': 'already_analyzed',
                        'session_id': str(recent_session.session_id),
                        'result': recent_session.analysis_result,
                        'message': '1æ™‚é–“ä»¥å†…ã«åˆ†ææ¸ˆã¿ã§ã™'
                    }
            
            session = SentimentAnalysisSession.objects.create(
                document=document,
                processing_status='PENDING'
            )
            
            threading.Thread(
                target=self._execute_analysis,
                args=(session.id, user_ip),
                daemon=True
            ).start()
            
            return {
                'status': 'started',
                'session_id': str(session.session_id),
                'message': 'è©³ç´°ãªè¦‹è§£ã‚’å«ã‚€æ„Ÿæƒ…åˆ†æã‚’é–‹å§‹ã—ã¾ã—ãŸ'
            }
            
        except DocumentMetadata.DoesNotExist:
            raise Exception('æŒ‡å®šã•ã‚ŒãŸæ›¸é¡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
        except Exception as e:
            logger.error(f"åˆ†æé–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
            raise Exception(f"åˆ†æé–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
    
    def get_progress(self, session_id: str) -> Dict[str, Any]:
        """é€²è¡ŒçŠ¶æ³å–å¾—"""
        from ..models import SentimentAnalysisSession
        
        try:
            session = SentimentAnalysisSession.objects.get(session_id=session_id)
            
            if session.is_expired:
                return {'status': 'expired', 'message': 'ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒæœŸé™åˆ‡ã‚Œã§ã™'}
            
            if session.processing_status == 'PROCESSING':
                result = session.analysis_result or {}
                progress = result.get('progress', 50)
                message = result.get('current_step', 'è©³ç´°è¦‹è§£ã‚’ç”Ÿæˆä¸­...')
            elif session.processing_status == 'COMPLETED':
                progress = 100
                message = 'è©³ç´°åˆ†æãƒ»è¦‹è§£ç”Ÿæˆå®Œäº†'
            elif session.processing_status == 'FAILED':
                progress = 100
                message = f'åˆ†æå¤±æ•—: {session.error_message}'
            else:
                progress = 0
                message = 'åˆ†æå¾…æ©Ÿä¸­...'
            
            return {
                'progress': progress,
                'message': message,
                'status': session.processing_status,
                'timestamp': timezone.now().isoformat()
            }
            
        except SentimentAnalysisSession.DoesNotExist:
            return {'status': 'not_found', 'message': 'ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}
    
    def get_result(self, session_id: str) -> Dict[str, Any]:
        """åˆ†æçµæœå–å¾—"""
        from ..models import SentimentAnalysisSession
        
        try:
            session = SentimentAnalysisSession.objects.get(session_id=session_id)
            
            if session.is_expired:
                return {'status': 'expired', 'message': 'ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒæœŸé™åˆ‡ã‚Œã§ã™'}
            
            if session.processing_status == 'COMPLETED':
                return {'status': 'completed', 'result': session.analysis_result}
            elif session.processing_status == 'FAILED':
                return {'status': 'failed', 'error': session.error_message}
            else:
                return {'status': 'processing', 'message': 'åˆ†æä¸­ã§ã™'}
                
        except SentimentAnalysisSession.DoesNotExist:
            return {'status': 'not_found', 'message': 'ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“'}
    
    def _execute_analysis(self, session_id: int, user_ip: str = None):
        """åˆ†æå®Ÿè¡Œï¼ˆè¦‹è§£ç”Ÿæˆå¼·åŒ–ç‰ˆï¼‰"""
        from ..models import SentimentAnalysisSession, SentimentAnalysisHistory
        
        start_time = time.time()
        
        try:
            session = SentimentAnalysisSession.objects.get(id=session_id)
            session.processing_status = 'PROCESSING'
            session.analysis_result = {'progress': 5, 'current_step': 'æ›¸é¡æƒ…å ±ç¢ºèªä¸­...'}
            session.save()
            
            # æ›¸é¡æƒ…å ±ã‚’æº–å‚™
            document_info = {
                'company_name': session.document.company_name,
                'doc_description': session.document.doc_description,
                'doc_type_code': session.document.doc_type_code,
                'submit_date': session.document.submit_date_time.strftime('%Y-%m-%d'),
                'securities_code': session.document.securities_code or '',
            }
            
            session.analysis_result = {'progress': 20, 'current_step': 'XBRLãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ä¸­...'}
            session.save()
            
            try:
                xbrl_text_sections = self.xbrl_service.get_xbrl_text_from_document(session.document)
            except Exception as e:
                logger.warning(f"XBRLå–å¾—å¤±æ•—: {e}")
                xbrl_text_sections = None
            
            if not xbrl_text_sections:
                session.analysis_result = {'progress': 40, 'current_step': 'åŸºæœ¬æƒ…å ±ã‚’ä½¿ç”¨ã—ã¦è©³ç´°åˆ†æä¸­...'}
                session.save()
                
                document_text = self._extract_basic_document_text(session.document)
                result = self.analyzer.analyze_text(document_text, str(session.session_id), document_info)
            else:
                session.analysis_result = {'progress': 50, 'current_step': 'XBRLãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ä¸­...'}
                session.save()
                
                session.analysis_result = {'progress': 70, 'current_step': 'è©³ç´°æ„Ÿæƒ…åˆ†æå®Ÿè¡Œä¸­...'}
                session.save()
                
                result = self.analyzer.analyze_text_sections(xbrl_text_sections, str(session.session_id), document_info)
            
            session.analysis_result = {'progress': 90, 'current_step': 'ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘è¦‹è§£ç”Ÿæˆä¸­...'}
            session.save()
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ›´æ–°
            session.overall_score = result['overall_score']
            session.sentiment_label = result['sentiment_label']
            session.analysis_result = result
            session.processing_status = 'COMPLETED'
            session.save()
            
            # å±¥æ­´ä¿å­˜
            analysis_duration = time.time() - start_time
            SentimentAnalysisHistory.objects.create(
                document=session.document,
                overall_score=result['overall_score'],
                sentiment_label=result['sentiment_label'],
                user_ip=user_ip,
                analysis_duration=analysis_duration
            )
            
            logger.info(f"è¦‹è§£ç”Ÿæˆä»˜ãæ„Ÿæƒ…åˆ†æå®Œäº†: {session.session_id} ({analysis_duration:.2f}ç§’)")
            
        except Exception as e:
            logger.error(f"æ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼: {session_id} - {e}")
            
            try:
                session = SentimentAnalysisSession.objects.get(id=session_id)
                session.processing_status = 'FAILED'
                session.error_message = str(e)
                session.save()
            except:
                pass
    
    def _extract_basic_document_text(self, document) -> str:
        """åŸºæœ¬çš„ãªæ›¸é¡æƒ…å ±ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        text_parts = [
            f"ä¼æ¥­å: {document.company_name}",
            f"æ›¸é¡æ¦‚è¦: {document.doc_description}",
            f"æå‡ºæ—¥: {document.submit_date_time.strftime('%Yå¹´%mæœˆ%dæ—¥')}",
        ]
        
        if document.period_start and document.period_end:
            text_parts.append(f"å¯¾è±¡æœŸé–“: {document.period_start}ã‹ã‚‰{document.period_end}")
        
        # ã‚ˆã‚Šç¾å®Ÿçš„ãªã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå¤šãã®èªå½™ã‚’å«ã‚€ï¼‰
        sample_scenarios = [
            "å½“ç¤¾ã®æ¥­ç¸¾ã¯å‰å¹´åŒæœŸã¨æ¯”è¼ƒã—ã¦é †èª¿ã«æ¨ç§»ã—ã¦ãŠã‚Šã€å£²ä¸Šé«˜ã®å¢—åŠ ã¨åç›Šæ€§ã®å‘ä¸ŠãŒå®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™ã€‚",
            "ä¸€æ–¹ã§ã€æ¸›åå¹…ã®ç¸®å°ã‚‚è¦‹ã‚‰ã‚Œã€å¸‚å ´ç’°å¢ƒã®å¤‰åŒ–ã«é©å¿œã—ã¤ã¤ç¶™ç¶šçš„ãªäº‹æ¥­æ”¹å–„ã‚’å›³ã£ã¦ã„ã¾ã™ã€‚", 
            "å–¶æ¥­æå¤±ã¯ç™ºç”Ÿã—ãŸã‚‚ã®ã®ã€æå¤±ã®æ”¹å–„å‚¾å‘ãŒè¦‹ã‚‰ã‚Œã€ä»Šå¾Œã®å›å¾©ã«æœŸå¾…ã—ã¦ã„ã¾ã™ã€‚",
            "ä»Šå¾Œã‚‚æŒç¶šçš„ãªæˆé•·ã‚’ç›®æŒ‡ã—ã€åŠ¹ç‡çš„ãªçµŒå–¶è³‡æºã®æ´»ç”¨ã¨ç«¶äº‰åŠ›ã®å¼·åŒ–ã«å–ã‚Šçµ„ã‚“ã§ã¾ã„ã‚Šã¾ã™ã€‚",
            "ä¸€éƒ¨ã®äº‹æ¥­ã§ã¯è‹¦æˆ¦ãŒç¶šã„ã¦ã„ã¾ã™ãŒã€å…¨ä½“ã¨ã—ã¦ã¯å¥½èª¿ãªæ¥­ç¸¾ã‚’ç¶­æŒã—ã¦ã„ã¾ã™ã€‚",
            "å¢—åå¢—ç›Šã‚’é”æˆã—ã€æ ªä¸»ã®çš†æ§˜ã«ã¯æ·±ãæ„Ÿè¬ç”³ã—ä¸Šã’ã¾ã™ã€‚",
            "æ¸›ç›Šã¨ãªã‚Šã¾ã—ãŸãŒã€æ§‹é€ æ”¹é©ã®åŠ¹æœã«ã‚ˆã‚Šä»Šå¾Œã®æ¥­ç¸¾å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚",
            "èµ¤å­—ç¸®å°ã«ã‚ˆã‚Šé»’å­—è»¢æ›ã¸ã®é“ç­‹ãŒè¦‹ãˆã¦ãã¾ã—ãŸã€‚",
            "Vå­—å›å¾©ã‚’ç›®æŒ‡ã—ã€æŠœæœ¬çš„ãªæ”¹é©ã«å–ã‚Šçµ„ã‚“ã§ãŠã‚Šã¾ã™ã€‚"
        ]
        
        text_parts.extend(sample_scenarios)
        return " ".join(text_parts)
    
    def cleanup_expired_sessions(self) -> int:
        """æœŸé™åˆ‡ã‚Œã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        from ..models import SentimentAnalysisSession
        
        try:
            expired_count = SentimentAnalysisSession.objects.filter(
                expires_at__lt=timezone.now()
            ).delete()[0]
            
            logger.info(f"æœŸé™åˆ‡ã‚Œã‚»ãƒƒã‚·ãƒ§ãƒ³å‰Šé™¤: {expired_count}ä»¶")
            return expired_count
            
        except Exception as e:
            logger.error(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")
            return 0


    def debug_zip_structure(self, zip_content: bytes, doc_id: str = None):
        """ZIPãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã®è©³ç´°ãƒ‡ãƒãƒƒã‚°"""
        try:
            import zipfile
            import io
            
            logger.info(f"=== ZIPæ§‹é€ ãƒ‡ãƒãƒƒã‚°é–‹å§‹: {doc_id} ===")
            
            with zipfile.ZipFile(io.BytesIO(zip_content)) as zip_file:
                logger.info(f"ç·ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(zip_file.filelist)}")
                
                # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆ
                xbrl_files = []
                other_files = []
                
                for file_info in zip_file.filelist:
                    file_details = {
                        'filename': file_info.filename,
                        'size': file_info.file_size,
                        'compressed_size': file_info.compress_size,
                        'date_time': file_info.date_time,
                    }
                    
                    if file_info.filename.endswith('.xbrl'):
                        xbrl_files.append(file_details)
                    else:
                        other_files.append(file_details)
                
                logger.info(f"\nXBRLãƒ•ã‚¡ã‚¤ãƒ« ({len(xbrl_files)}å€‹):")
                for i, file_info in enumerate(xbrl_files):
                    logger.info(f"  {i+1}. {file_info['filename']}")
                    logger.info(f"     ã‚µã‚¤ã‚º: {file_info['size']:,} bytes")
                    logger.info(f"     æ—¥æ™‚: {'-'.join(map(str, file_info['date_time']))}")
                
                logger.info(f"\nãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ« ({len(other_files)}å€‹):")
                for file_info in other_files[:10]:  # æœ€åˆã®10å€‹ã®ã¿
                    logger.info(f"  - {file_info['filename']} ({file_info['size']:,} bytes)")
                
                # å„XBRLãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç°¡æ˜“åˆ†æ
                if xbrl_files:
                    logger.info(f"\n=== XBRLãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹åˆ†æ ===")
                    
                    for i, file_info in enumerate(xbrl_files):
                        filename = file_info['filename']
                        logger.info(f"\n--- {filename} ã®åˆ†æ ---")
                        
                        try:
                            with zip_file.open(filename) as xbrl_file:
                                xbrl_content = xbrl_file.read()
                                
                                # XMLã¨ã—ã¦è§£æ
                                import xml.etree.ElementTree as ET
                                root = ET.fromstring(xbrl_content)
                                
                                # åŸºæœ¬æƒ…å ±
                                logger.info(f"  ãƒ«ãƒ¼ãƒˆè¦ç´ : {root.tag}")
                                
                                # åå‰ç©ºé–“ã®ç¢ºèª
                                namespaces = self._extract_namespaces(root)
                                logger.info(f"  åå‰ç©ºé–“æ•°: {len(namespaces)}")
                                for prefix, uri in list(namespaces.items())[:5]:
                                    logger.info(f"    {prefix}: {uri}")
                                
                                # è²¡å‹™é–¢é€£è¦ç´ ã®å­˜åœ¨ç¢ºèª
                                cf_elements = self._count_financial_elements(root)
                                logger.info(f"  è²¡å‹™è¦ç´ æ•°:")
                                for element_type, count in cf_elements.items():
                                    logger.info(f"    {element_type}: {count}å€‹")
                                
                                # ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã®ç¢ºèª
                                text_elements = self._count_text_elements(root)
                                logger.info(f"  ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ : {text_elements}å€‹")
                                
                                # ãƒ‡ãƒ¼ã‚¿å“è³ªã®äºˆå‚™è©•ä¾¡
                                quality = self._quick_quality_assessment(root)
                                logger.info(f"  å“è³ªã‚¹ã‚³ã‚¢ï¼ˆäºˆå‚™ï¼‰: {quality:.3f}")
                                
                        except Exception as e:
                            logger.error(f"  ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            
            logger.info("=== ZIPæ§‹é€ ãƒ‡ãƒãƒƒã‚°çµ‚äº† ===")
            
        except Exception as e:
            logger.error(f"ZIPæ§‹é€ ãƒ‡ãƒãƒƒã‚°ã‚¨ãƒ©ãƒ¼: {e}")

    def _extract_namespaces(self, root):
        """XMLåå‰ç©ºé–“ã®æŠ½å‡º"""
        namespaces = {}
        for key, value in root.attrib.items():
            if key.startswith('xmlns'):
                prefix = key.split(':')[1] if ':' in key else 'default'
                namespaces[prefix] = value
        return namespaces

    def _count_financial_elements(self, root):
        """è²¡å‹™é–¢é€£è¦ç´ ã®ã‚«ã‚¦ãƒ³ãƒˆ"""
        counts = {
            'operating_cf': 0,
            'investing_cf': 0,
            'financing_cf': 0,
            'sales': 0,
            'assets': 0,
        }
        
        financial_patterns = {
            'operating_cf': ['OperatingActivities', 'OperatingCashFlow', 'å–¶æ¥­æ´»å‹•'],
            'investing_cf': ['InvestingActivities', 'InvestingCashFlow', 'æŠ•è³‡æ´»å‹•'],
            'financing_cf': ['FinancingActivities', 'FinancingCashFlow', 'è²¡å‹™æ´»å‹•'],
            'sales': ['NetSales', 'Sales', 'Revenue', 'å£²ä¸Š'],
            'assets': ['TotalAssets', 'Assets', 'è³‡ç”£'],
        }
        
        for elem in root.iter():
            elem_text = elem.tag + (elem.text or '')
            
            for category, patterns in financial_patterns.items():
                for pattern in patterns:
                    if pattern in elem_text:
                        counts[category] += 1
                        break
        
        return counts

    def _count_text_elements(self, root):
        """é•·ã„ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã®ã‚«ã‚¦ãƒ³ãƒˆ"""
        count = 0
        for elem in root.iter():
            if elem.text and len(elem.text.strip()) > 100:
                count += 1
        return count

    def _quick_quality_assessment(self, root):
        """ã‚¯ã‚¤ãƒƒã‚¯å“è³ªè©•ä¾¡"""
        financial_counts = self._count_financial_elements(root)
        text_count = self._count_text_elements(root)
        
        # è²¡å‹™è¦ç´ ã®å®Œå…¨æ€§
        financial_score = sum(1 for count in financial_counts.values() if count > 0) / len(financial_counts)
        
        # ãƒ†ã‚­ã‚¹ãƒˆã®è±Šå¯Œã•
        text_score = min(text_count / 10, 1.0)
        
        return financial_score * 0.7 + text_score * 0.3

    # EDINETXBRLService ã‚¯ãƒ©ã‚¹ã«è¿½åŠ ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰
    def debug_zip_document(self, document):
        """ç‰¹å®šæ›¸é¡ã®ZIPæ§‹é€ ã‚’ãƒ‡ãƒãƒƒã‚°"""
        try:
            from .edinet_api import EdinetAPIClient
            api_client = EdinetAPIClient.create_v2_client()
            
            logger.info(f"ZIPæ§‹é€ ãƒ‡ãƒãƒƒã‚°é–‹å§‹: {document.doc_id}")
            xbrl_data = api_client.get_document(document.doc_id, doc_type=1)
            
            if xbrl_data[:4] == b'PK\x03\x04':
                self.extractor.debug_zip_structure(xbrl_data, document.doc_id)
            else:
                logger.info(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ã‚ã‚Šã¾ã›ã‚“: {document.doc_id}")
            
            return {'status': 'debug_completed'}
            
        except Exception as e:
            logger.error(f"ZIPæ§‹é€ ãƒ‡ãƒãƒƒã‚°ã‚¨ãƒ©ãƒ¼: {document.doc_id} - {e}")
            return {'status': 'debug_failed', 'error': str(e)}

    def compare_extraction_methods(self, document):
        """æ–°æ—§ã®æŠ½å‡ºæ–¹æ³•ã‚’æ¯”è¼ƒ"""
        try:
            from .edinet_api import EdinetAPIClient
            api_client = EdinetAPIClient.create_v2_client()
            
            logger.info(f"æŠ½å‡ºæ–¹æ³•æ¯”è¼ƒé–‹å§‹: {document.doc_id}")
            xbrl_data = api_client.get_document(document.doc_id, doc_type=1)
            
            if xbrl_data[:4] == b'PK\x03\x04':
                # æ–°ã—ã„æ–¹æ³•
                logger.info("=== æ–°ã—ã„æ–¹æ³•ï¼ˆãƒ‡ãƒ¼ã‚¿æ¬ æå¯¾ç­–ç‰ˆï¼‰===")
                new_result = self._extract_comprehensive_from_bytes_safe(xbrl_data, document.doc_id)
                
                logger.info(f"æ–°æ–¹æ³•çµæœ:")
                logger.info(f"  è²¡å‹™ãƒ‡ãƒ¼ã‚¿: {len(new_result.get('financial_data', {}))}é …ç›®")
                logger.info(f"  ãƒ†ã‚­ã‚¹ãƒˆ: {len(new_result.get('text_sections', {}))}ã‚»ã‚¯ã‚·ãƒ§ãƒ³")
                if new_result.get('source_files'):
                    logger.info(f"  ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(new_result['source_files'])}")
                    for src in new_result['source_files']:
                        logger.info(f"    - {src['filename']}: å“è³ª{src['quality']}, è²¡å‹™{src['financial_items']}é …ç›®")
                
                # è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã®è©³ç´°
                logger.info("  è²¡å‹™ãƒ‡ãƒ¼ã‚¿è©³ç´°:")
                for key, value in new_result.get('financial_data', {}).items():
                    logger.info(f"    {key}: {value}")
                
                return new_result
            else:
                logger.info("ZIPãƒ•ã‚¡ã‚¤ãƒ«ã§ã¯ãªã„ãŸã‚æ¯”è¼ƒä¸è¦")
                return {'status': 'not_zip'}
            
        except Exception as e:
            logger.error(f"æŠ½å‡ºæ–¹æ³•æ¯”è¼ƒã‚¨ãƒ©ãƒ¼: {document.doc_id} - {e}")
            return {'status': 'comparison_failed', 'error': str(e)}        
            
            
    def _analyze_keyword_frequency(self, all_matches: List[Tuple[str, float, str]]) -> Dict[str, List[Dict]]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾é »åº¦ã®è©³ç´°åˆ†æ"""
        frequency_data = {'positive': [], 'negative': []}
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å‡ºç¾å›æ•°ã‚’é›†è¨ˆ
        keyword_counts = {}
        keyword_scores = {}
        keyword_types = {}
        
        for word, score, type_name in all_matches:
            if word not in keyword_counts:
                keyword_counts[word] = 0
                keyword_scores[word] = score
                keyword_types[word] = type_name
            
            keyword_counts[word] += 1
            # ã‚¹ã‚³ã‚¢ã¯å¹³å‡ã‚’å–ã‚‹
            keyword_scores[word] = (keyword_scores[word] + score) / 2
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ãƒ»ãƒã‚¬ãƒ†ã‚£ãƒ–ã«åˆ†é¡
        for word, count in keyword_counts.items():
            score = keyword_scores[word]
            
            keyword_data = {
                'word': word,
                'count': count,
                'score': score,
                'type': keyword_types[word],
                'impact_level': self._get_impact_level(score),
                'frequency_rank': 0  # å¾Œã§è¨­å®š
            }
            
            if score > 0:
                frequency_data['positive'].append(keyword_data)
            elif score < 0:
                frequency_data['negative'].append(keyword_data)
        
        # å‡ºç¾å›æ•°ã§ã‚½ãƒ¼ãƒˆã—ã¦ãƒ©ãƒ³ã‚¯ä»˜ã‘
        frequency_data['positive'].sort(key=lambda x: x['count'], reverse=True)
        frequency_data['negative'].sort(key=lambda x: x['count'], reverse=True)
        
        # ãƒ©ãƒ³ã‚¯ä»˜ã‘
        for i, item in enumerate(frequency_data['positive']):
            item['frequency_rank'] = i + 1
        
        for i, item in enumerate(frequency_data['negative']):
            item['frequency_rank'] = i + 1
        
        return frequency_data


    # analyze_text ãƒ¡ã‚½ãƒƒãƒ‰ã§ã®ãƒ‡ãƒãƒƒã‚°æœ‰åŠ¹åŒ–
    def analyze_text(self, text: str, session_id: str = None, document_info: Dict[str, str] = None) -> Dict[str, Any]:
        """é€æ˜æ€§ã®é«˜ã„æ„Ÿæƒ…åˆ†æï¼ˆæ–¹æ³•1ï¼šé‡è¤‡é‡ã¿ä»˜ã‘ç‰ˆï¼‰"""
        try:
            if not text or len(text.strip()) < 10:
                return self._empty_result(session_id)
            
            # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
            cleaned_text = self.text_processor.preprocess(text)
            
            # æ®µéšçš„ãªåˆ†æãƒ—ãƒ­ã‚»ã‚¹
            analysis_steps = []
            
            # ã‚¹ãƒ†ãƒƒãƒ—1: æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
            context_matches = self._find_context_patterns(cleaned_text)
            if context_matches:
                analysis_steps.append({
                    'step': 'æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º',
                    'description': 'ã€Œæ¸›åã®æ”¹å–„ã€ã€Œæˆé•·ã®éˆåŒ–ã€ã®ã‚ˆã†ãªæ–‡è„ˆã‚’è€ƒæ…®ã—ãŸè¡¨ç¾ã‚’æ¤œå‡º',
                    'matches': context_matches,
                    'impact': sum(score for _, score, _ in context_matches)
                })
            
            # ã‚¹ãƒ†ãƒƒãƒ—2: åŸºæœ¬èªå½™ã®æ¤œå‡º
            basic_matches = self._find_basic_words(cleaned_text, context_matches)
            if basic_matches:
                analysis_steps.append({
                    'step': 'åŸºæœ¬èªå½™æ¤œå‡º',
                    'description': 'æ„Ÿæƒ…è¾æ›¸ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹èªå½™ã‚’æ¤œå‡º',
                    'matches': basic_matches,
                    'impact': sum(score for _, score, _ in basic_matches)
                })
            
            # å…¨ã¦ã®ãƒãƒƒãƒã‚’çµ±åˆ
            all_matches = context_matches + basic_matches
            
            # ğŸ” ãƒ‡ãƒãƒƒã‚°ï¼šé‡è¤‡é‡ã¿ä»˜ã‘ã®åŠ¹æœç¢ºèª
            if all_matches:
                self.debug_method1_effect(all_matches)
            
            # ğŸ“Š æ–¹æ³•1ï¼šé‡è¤‡é‡ã¿ä»˜ã‘æ–¹å¼ã§ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
            score_calculation = self._calculate_detailed_score(all_matches)
            
            # å…¨ä½“ã‚¹ã‚³ã‚¢ã¨åˆ¤å®š
            overall_score = score_calculation['final_score']
            sentiment_label = self._determine_sentiment_label(overall_score)
            
            # åˆ†ææ ¹æ‹ ã®ç”Ÿæˆ
            analysis_reasoning = self._generate_reasoning(
                analysis_steps, score_calculation, overall_score, sentiment_label
            )
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
            keyword_analysis = self._analyze_keywords(all_matches)
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦åˆ†æ
            keyword_frequency_data = self._analyze_keyword_frequency_safe(all_matches)
            
            # æ–‡ç« ãƒ¬ãƒ™ãƒ«åˆ†æ
            sentences = self._split_sentences(cleaned_text)
            sentence_analysis = self._analyze_sentences(sentences)
            
            # åŸºæœ¬çµæœã®æ§‹ç¯‰
            basic_result = {
                'overall_score': round(overall_score, 3),
                'sentiment_label': sentiment_label,
                'analysis_reasoning': analysis_reasoning,
                'score_calculation': score_calculation,
                'analysis_steps': analysis_steps,
                'keyword_analysis': keyword_analysis,
                'keyword_frequency_data': keyword_frequency_data,
                'sample_sentences': {
                    'positive': [s for s in sentence_analysis if s['score'] > self.config.positive_threshold][:5],
                    'negative': [s for s in sentence_analysis if s['score'] < self.config.negative_threshold][:5],
                },
                'statistics': {
                    'total_words_analyzed': len(all_matches),
                    'unique_words_found': score_calculation.get('unique_words_count', 0),
                    'total_occurrences': score_calculation.get('total_occurrences', len(all_matches)),
                    'repetition_factor': score_calculation.get('repetition_factor', 1.0),
                    'context_patterns_found': len(context_matches),
                    'basic_words_found': len(basic_matches),
                    'sentences_analyzed': len(sentences),
                    'positive_words_count': len([s for _, s, _ in all_matches if s > 0]),
                    'negative_words_count': len([s for _, s, _ in all_matches if s < 0]),
                    'positive_sentences_count': len([s for s in sentence_analysis if s['score'] > self.config.positive_threshold]),
                    'negative_sentences_count': len([s for s in sentence_analysis if s['score'] < self.config.negative_threshold]),
                    'threshold_positive': self.config.positive_threshold,
                    'threshold_negative': self.config.negative_threshold,
                    'total_keyword_occurrences': sum(item['count'] for item in keyword_frequency_data['positive'] + keyword_frequency_data['negative']),
                    'top_positive_keyword': keyword_frequency_data['positive'][0] if keyword_frequency_data['positive'] else None,
                    'top_negative_keyword': keyword_frequency_data['negative'][0] if keyword_frequency_data['negative'] else None,
                    
                    # é‡è¤‡é‡ã¿ä»˜ã‘ç‰¹æœ‰ã®çµ±è¨ˆ
                    'weighted_positive_sum': score_calculation.get('weighted_positive_sum', 0),
                    'weighted_negative_sum': score_calculation.get('weighted_negative_sum', 0),
                    'weighted_total_sum': score_calculation.get('weighted_total_sum', 0),
                },
                'analysis_metadata': {
                    'analyzed_at': timezone.now().isoformat(),
                    'dictionary_size': len(self.dictionary.sentiment_dict),
                    'session_id': session_id,
                    'analysis_version': '3.1_repetition_weighted',
                    'calculation_method': 'repetition_weighted',
                    'weight_formula': '1å›:1.0å€, 2å›:1.8å€, 3å›:2.4å€, 4å›ä»¥é™:+0.4å€ãšã¤'
                }
            }
            
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘è©³ç´°è¦‹è§£ã‚’ç”Ÿæˆ
            if document_info:
                user_insights = self.insight_generator.generate_detailed_insights(basic_result, document_info)
                basic_result['user_insights'] = user_insights
            
            return basic_result
            
        except Exception as e:
            logger.error(f"æ„Ÿæƒ…åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            raise Exception(f"æ„Ÿæƒ…åˆ†æå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")

