# management/commands/import_jpx_split_batch.py
import requests
import pdfplumber
import re
import tempfile
import os
import gc
import warnings
import time
import subprocess
import sys
from datetime import datetime, date
from django.core.management.base import BaseCommand, CommandError
from django.db import transaction, connection
from django.conf import settings
from margin_trading.models import MarketIssue, MarginTradingData, DataImportLog

# è­¦å‘ŠæŠ‘åˆ¶
warnings.filterwarnings('ignore')
import logging
logging.getLogger('pdfplumber').setLevel(logging.ERROR)

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

class Command(BaseCommand):
    help = 'åˆ†å‰²ãƒãƒƒãƒå‡¦ç†ã§JPXãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆãƒ¡ãƒ¢ãƒªå®‰å…¨ç‰ˆï¼‰'

    def add_arguments(self, parser):
        parser.add_argument('--date', type=str, help='å–å¾—å¯¾è±¡æ—¥ä»˜ (YYYYMMDDå½¢å¼)')
        parser.add_argument('--force', action='store_true', help='å¼·åˆ¶å®Ÿè¡Œ')
        parser.add_argument('--pages-per-batch', type=int, default=10, help='ãƒãƒƒãƒã‚ãŸã‚Šã®ãƒšãƒ¼ã‚¸æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 10ï¼‰')
        parser.add_argument('--batch-size', type=int, default=20, help='ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 20ï¼‰')
        parser.add_argument('--cleanup-interval', type=int, default=30, help='ãƒãƒƒãƒé–“ã®å¾…æ©Ÿç§’æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 30ï¼‰')
        parser.add_argument('--coordinator', action='store_true', help='ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ï¼ˆåˆ†å‰²å®Ÿè¡Œç®¡ç†ï¼‰ãƒ¢ãƒ¼ãƒ‰')
        parser.add_argument('--worker', action='store_true', help='ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼ˆå€‹åˆ¥å‡¦ç†ï¼‰ãƒ¢ãƒ¼ãƒ‰')
        parser.add_argument('--start-page', type=int, help='é–‹å§‹ãƒšãƒ¼ã‚¸ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰')
        parser.add_argument('--end-page', type=int, help='çµ‚äº†ãƒšãƒ¼ã‚¸ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰')
        parser.add_argument('--pdf-path', type=str, help='PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰')

    def handle(self, *args, **options):
        target_date = options.get('date')
        force = options.get('force', False)
        
        if target_date:
            try:
                target_date = datetime.strptime(target_date, '%Y%m%d').date()
            except ValueError:
                raise CommandError('æ—¥ä»˜ã¯ YYYYMMDD å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„')
        else:
            target_date = date.today()
        
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆ
        if options.get('worker'):
            return self._worker_mode(options, target_date)
        
        # ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
        return self._coordinator_mode(options, target_date, force)

    def _coordinator_mode(self, options, target_date, force):
        """ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ã‚¿ãƒ¼ï¼šåˆ†å‰²å®Ÿè¡Œã‚’ç®¡ç†"""
        pages_per_batch = options.get('pages_per_batch', 10)
        cleanup_interval = options.get('cleanup_interval', 30)
        batch_size = options.get('batch_size', 20)
        
        self.stdout.write("=" * 60)
        self.stdout.write(f"ğŸš€ åˆ†å‰²ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {target_date}")
        self.stdout.write(f"ğŸ“Š è¨­å®š: {pages_per_batch}ãƒšãƒ¼ã‚¸/ãƒãƒƒãƒ, {cleanup_interval}ç§’é–“éš”")
        self.stdout.write("=" * 60)
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
        existing_count = MarginTradingData.objects.filter(date=target_date).count()
        if not force and existing_count > 0:
            self.stdout.write(
                self.style.WARNING(f'æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ {existing_count}ä»¶ã‚ã‚Šã€‚--force ã§ä¸Šæ›¸ãå¯èƒ½'))
            return
        
        # PDFå–å¾—
        pdf_url = self._generate_pdf_url(target_date)
        pdf_path = self._download_pdf(pdf_url)
        
        try:
            # ç·ãƒšãƒ¼ã‚¸æ•°å–å¾—
            total_pages = self._get_total_pages(pdf_path)
            self.stdout.write(f"ğŸ“„ ç·ãƒšãƒ¼ã‚¸æ•°: {total_pages}")
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ï¼ˆforceæ™‚ï¼‰
            if force and existing_count > 0:
                MarginTradingData.objects.filter(date=target_date).delete()
                self.stdout.write(f"ğŸ—‘ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ {existing_count}ä»¶å‰Šé™¤")
            
            # ãƒãƒƒãƒå®Ÿè¡Œè¨ˆç”»
            batches = []
            for start_page in range(0, total_pages, pages_per_batch):
                end_page = min(start_page + pages_per_batch, total_pages)
                batches.append((start_page, end_page))
            
            self.stdout.write(f"ğŸ“¦ å®Ÿè¡Œäºˆå®š: {len(batches)}ãƒãƒƒãƒ")
            for i, (start, end) in enumerate(batches):
                self.stdout.write(f"  ãƒãƒƒãƒ{i+1}: ãƒšãƒ¼ã‚¸{start+1}-{end}")
            
            # ãƒãƒƒãƒå®Ÿè¡Œ
            total_records = 0
            success_batches = 0
            
            for batch_num, (start_page, end_page) in enumerate(batches, 1):
                self.stdout.write(f"\nğŸ”„ ãƒãƒƒãƒ{batch_num}/{len(batches)} å®Ÿè¡Œä¸­...")
                self.stdout.write(f"ğŸ“„ å¯¾è±¡: ãƒšãƒ¼ã‚¸{start_page+1}-{end_page}")
                
                try:
                    # ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
                    records = self._execute_worker_batch(
                        pdf_path, target_date, start_page, end_page, batch_size
                    )
                    
                    total_records += records
                    success_batches += 1
                    
                    self.stdout.write(f"âœ… ãƒãƒƒãƒ{batch_num}å®Œäº†: {records}ä»¶å–å¾—")
                    self.stdout.write(f"ğŸ“ˆ ç´¯è¨ˆ: {total_records}ä»¶")
                    
                    # ãƒãƒƒãƒé–“å¾…æ©Ÿï¼ˆã‚·ã‚¹ãƒ†ãƒ å›å¾©ï¼‰
                    if batch_num < len(batches):
                        self.stdout.write(f"â³ {cleanup_interval}ç§’å¾…æ©Ÿä¸­...")
                        time.sleep(cleanup_interval)
                        
                        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
                        if PSUTIL_AVAILABLE:
                            self._log_system_status()
                
                except Exception as e:
                    self.stdout.write(f"âŒ ãƒãƒƒãƒ{batch_num}ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue
            
            # çµæœå ±å‘Š
            self.stdout.write("\n" + "=" * 60)
            self.stdout.write(f"ğŸ‰ åˆ†å‰²ãƒãƒƒãƒå‡¦ç†å®Œäº†!")
            self.stdout.write(f"âœ… æˆåŠŸãƒãƒƒãƒ: {success_batches}/{len(batches)}")
            self.stdout.write(f"ğŸ“Š ç·å–å¾—ä»¶æ•°: {total_records}ä»¶")
            
            # ãƒ­ã‚°è¨˜éŒ²
            DataImportLog.objects.create(
                date=target_date,
                status='SUCCESS',
                message=f'åˆ†å‰²ãƒãƒƒãƒã§{total_records}ä»¶å–å¾—ï¼ˆ{success_batches}/{len(batches)}ãƒãƒƒãƒæˆåŠŸï¼‰',
                records_count=total_records,
                pdf_url=pdf_url
            )
            
        finally:
            # PDFå‰Šé™¤
            os.unlink(pdf_path)

    def _worker_mode(self, options, target_date):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼šæŒ‡å®šãƒšãƒ¼ã‚¸ç¯„å›²ã®ã¿å‡¦ç†"""
        pdf_path = options.get('pdf_path')
        start_page = options.get('start_page')
        end_page = options.get('end_page')
        batch_size = options.get('batch_size', 20)
        
        if not all([pdf_path, start_page is not None, end_page is not None]):
            raise CommandError('ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ¢ãƒ¼ãƒ‰ã«ã¯ --pdf-path, --start-page, --end-page ãŒå¿…è¦')
        
        self.stdout.write(f"ğŸ”§ ãƒ¯ãƒ¼ã‚«ãƒ¼é–‹å§‹: ãƒšãƒ¼ã‚¸{start_page+1}-{end_page}")
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ­ã‚°
        self._log_memory("ãƒ¯ãƒ¼ã‚«ãƒ¼é–‹å§‹")
        
        records_count = 0
        batch_data = []
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for page_num in range(start_page, min(end_page, len(pdf.pages))):
                    self.stdout.write(f"ğŸ“„ ãƒšãƒ¼ã‚¸{page_num+1}å‡¦ç†ä¸­...")
                    
                    page = pdf.pages[page_num]
                    
                    try:
                        tables = page.extract_tables()
                        if tables:
                            for table in tables:
                                for row in table:
                                    if self._is_data_row(row):
                                        try:
                                            data_dict = self._parse_data_row(row)
                                            batch_data.append(data_dict)
                                            
                                            # ãƒãƒƒãƒä¿å­˜
                                            if len(batch_data) >= batch_size:
                                                self._save_batch(batch_data, target_date)
                                                records_count += len(batch_data)
                                                batch_data = []
                                                gc.collect()
                                                
                                        except Exception as e:
                                            continue
                    except Exception as e:
                        self.stdout.write(f"âš ï¸ ãƒšãƒ¼ã‚¸{page_num+1}ã§ã‚¨ãƒ©ãƒ¼: {str(e)}")
                        continue
                    
                    # ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                    del page
                    gc.collect()
            
            # æ®‹ã‚Šãƒ‡ãƒ¼ã‚¿ä¿å­˜
            if batch_data:
                self._save_batch(batch_data, target_date)
                records_count += len(batch_data)
            
            self._log_memory("ãƒ¯ãƒ¼ã‚«ãƒ¼å®Œäº†")
            self.stdout.write(f"âœ… ãƒ¯ãƒ¼ã‚«ãƒ¼å®Œäº†: {records_count}ä»¶")
            
            # çµæœã‚’æ¨™æº–å‡ºåŠ›ã«å‡ºåŠ›ï¼ˆè¦ªãƒ—ãƒ­ã‚»ã‚¹ãŒèª­ã¿å–ã‚Šï¼‰
            print(f"WORKER_RESULT:{records_count}")
            
        except Exception as e:
            self.stdout.write(f"âŒ ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¨ãƒ©ãƒ¼: {str(e)}")
            print("WORKER_RESULT:0")
            raise

    def _execute_worker_batch(self, pdf_path, target_date, start_page, end_page, batch_size):
        """ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œ"""
        cmd = [
            sys.executable, 'manage.py', 'import_jpx_margin_data',
            '--worker',
            '--pdf-path', pdf_path,
            '--start-page', str(start_page),
            '--end-page', str(end_page),
            '--batch-size', str(batch_size),
            '--date', target_date.strftime('%Y%m%d')
        ]
        
        self.stdout.write(f"ğŸš€ ãƒ¯ãƒ¼ã‚«ãƒ¼å®Ÿè¡Œ: {' '.join(cmd[-8:])}")
        
        # ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            universal_newlines=True
        )
        
        # å‡ºåŠ›ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
        records_count = 0
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                line = output.strip()
                if line.startswith('WORKER_RESULT:'):
                    records_count = int(line.split(':')[1])
                else:
                    self.stdout.write(f"  {line}")
        
        return_code = process.poll()
        if return_code != 0:
            raise Exception(f"ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ãŒå¤±æ•—ã—ã¾ã—ãŸï¼ˆçµ‚äº†ã‚³ãƒ¼ãƒ‰: {return_code}ï¼‰")
        
        return records_count

    def _download_pdf(self, pdf_url):
        """PDF ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        self.stdout.write('ğŸ“¥ PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...')
        
        response = requests.get(pdf_url, timeout=60, stream=True)
        response.raise_for_status()
        
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            total_size = 0
            for chunk in response.iter_content(chunk_size=8192):
                tmp_file.write(chunk)
                total_size += len(chunk)
            
            self.stdout.write(f'âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {total_size/1024/1024:.1f}MB')
            return tmp_file.name

    def _get_total_pages(self, pdf_path):
        """ç·ãƒšãƒ¼ã‚¸æ•°å–å¾—"""
        with pdfplumber.open(pdf_path) as pdf:
            return len(pdf.pages)

    def _generate_pdf_url(self, target_date):
        """PDF URLã‚’ç”Ÿæˆ"""
        date_str = target_date.strftime('%Y%m%d')
        base_url = 'https://www.jpx.co.jp/markets/statistics-equities/margin/tvdivq0000001rnl-att/'
        filename = f'syumatsu{date_str}00.pdf'
        return f'{base_url}{filename}'

    def _is_data_row(self, row):
        """ãƒ‡ãƒ¼ã‚¿è¡Œã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        if not row or len(row) < 4:
            return False
        
        first_cell = str(row[0]) if row[0] else ''
        return (first_cell.startswith('B ') and 
                'æ™®é€šæ ªå¼' in first_cell and 
                len(row) >= 10)

    def _parse_data_row(self, row):
        """ãƒ‡ãƒ¼ã‚¿è¡Œã®è§£æ"""
        first_cell = str(row[0])
        
        match = re.match(r'B\s+(.+?)\s+æ™®é€šæ ªå¼\s+(\d+)', first_cell)
        if not match:
            raise ValueError(f'éŠ˜æŸ„æƒ…å ±ã®è§£æã«å¤±æ•—: {first_cell}')
        
        issue_name = match.group(1).strip()
        issue_code = match.group(2)
        jp_code = str(row[3]) if row[3] else ''
        
        numeric_values = []
        for i in range(4, len(row)):
            value = self._parse_numeric_value(row[i])
            numeric_values.append(value)
        
        while len(numeric_values) < 12:
            numeric_values.append(0)
        
        return {
            'issue_code': issue_code,
            'jp_code': jp_code,
            'issue_name': issue_name,
            'margin_data': {
                'outstanding_sales': numeric_values[0],
                'outstanding_sales_change': numeric_values[1],
                'outstanding_purchases': numeric_values[2],
                'outstanding_purchases_change': numeric_values[3],
                'negotiable_credit': numeric_values[4],
                'negotiable_credit_change': numeric_values[5],
                'standardized_credit': numeric_values[6],
                'standardized_credit_change': numeric_values[7],
                'additional_data_1': numeric_values[8] if len(numeric_values) > 8 else None,
                'additional_data_2': numeric_values[9] if len(numeric_values) > 9 else None,
                'additional_data_3': numeric_values[10] if len(numeric_values) > 10 else None,
                'additional_data_4': numeric_values[11] if len(numeric_values) > 11 else None,
            }
        }

    def _parse_numeric_value(self, value):
        """æ•°å€¤ã®è§£æ"""
        if not value or value == '-':
            return 0
        
        value_str = str(value).strip()
        is_negative = value_str.startswith('â–²')
        if is_negative:
            value_str = value_str[1:]
        
        try:
            value_str = value_str.replace(',', '')
            numeric_value = int(float(value_str))
            return -numeric_value if is_negative else numeric_value
        except (ValueError, TypeError):
            return 0

    def _save_batch(self, batch_data, target_date):
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
        if not batch_data:
            return
        
        with transaction.atomic():
            for data_dict in batch_data:
                try:
                    issue, created = MarketIssue.objects.get_or_create(
                        code=data_dict['issue_code'],
                        defaults={
                            'jp_code': data_dict['jp_code'],
                            'name': data_dict['issue_name'],
                            'category': 'B'
                        }
                    )
                    
                    MarginTradingData.objects.update_or_create(
                        issue=issue,
                        date=target_date,
                        defaults=data_dict['margin_data']
                    )
                    
                except Exception as e:
                    continue

    def _log_memory(self, label):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        if not PSUTIL_AVAILABLE:
            return
        
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            self.stdout.write(f'ğŸ“Š {label}: {memory_mb:.1f}MB')
        except:
            pass

    def _log_system_status(self):
        """ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ³ãƒ­ã‚°"""
        if not PSUTIL_AVAILABLE:
            return
        
        try:
            memory = psutil.virtual_memory()
            self.stdout.write(f'ğŸ’¾ ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒ¢ãƒª: {memory.available/1024/1024:.0f}MBåˆ©ç”¨å¯èƒ½ ({memory.percent:.1f}%ä½¿ç”¨ä¸­)')
        except:
            pass