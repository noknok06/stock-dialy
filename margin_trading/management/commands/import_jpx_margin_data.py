# management/commands/import_jpx_margin_data_improved.py
import requests
import pdfplumber
import re
import tempfile
import os
import gc
import warnings
import time
from datetime import datetime, date
from django.core.management.base import BaseCommand, CommandError
from django.db import transaction, connection
from django.conf import settings
from margin_trading.models import MarketIssue, MarginTradingData, DataImportLog

# PDFè­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings('ignore', category=UserWarning, module='pdfplumber')
warnings.filterwarnings('ignore', category=RuntimeWarning, module='pdfplumber')

# PDFã®ã‚«ãƒ©ãƒ¼å‡¦ç†è­¦å‘Šã‚’æŠ‘åˆ¶
import logging
logging.getLogger('pdfplumber').setLevel(logging.ERROR)

try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

class Command(BaseCommand):
    help = 'JPXã‹ã‚‰ä¿¡ç”¨å–å¼•ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ç‰ˆï¼‰'

    def add_arguments(self, parser):
        parser.add_argument(
            '--date',
            type=str,
            help='å–å¾—å¯¾è±¡æ—¥ä»˜ (YYYYMMDDå½¢å¼, çœç•¥æ™‚ã¯å½“æ—¥)',
        )
        parser.add_argument(
            '--force',
            action='store_true',
            help='æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã£ã¦ã‚‚å¼·åˆ¶çš„ã«å–å¾—ãƒ»æ›´æ–°',
        )
        parser.add_argument(
            '--memory-limit',
            type=int,
            default=128,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’128MBã«å‰Šæ¸›
            help='ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡åˆ¶é™ï¼ˆMBã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 128ï¼‰',
        )
        parser.add_argument(
            '--batch-size',
            type=int,
            default=25,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’25ã«å‰Šæ¸›
            help='ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 25ï¼‰',
        )
        parser.add_argument(
            '--page-interval',
            type=int,
            default=5,
            help='ä½•ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰',
        )
        parser.add_argument(
            '--aggressive-gc',
            action='store_true',
            help='ç©æ¥µçš„ãªã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’æœ‰åŠ¹ã«ã™ã‚‹',
        )

    def handle(self, *args, **options):
        target_date = options.get('date')
        force = options.get('force', False)
        batch_size = options.get('batch_size', 25)
        memory_limit = options.get('memory_limit', 128) * 1024 * 1024  # MB to bytes
        page_interval = options.get('page_interval', 5)
        aggressive_gc = options.get('aggressive_gc', False)
        
        self.batch_size = batch_size
        self.memory_limit = memory_limit
        self.page_interval = page_interval
        self.aggressive_gc = aggressive_gc
        
        if target_date:
            try:
                target_date = datetime.strptime(target_date, '%Y%m%d').date()
            except ValueError:
                raise CommandError('æ—¥ä»˜ã¯ YYYYMMDD å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„')
        else:
            target_date = date.today()
        
        self.stdout.write(f"ğŸš€ å‡¦ç†é–‹å§‹: {target_date}")
        self.stdout.write(f"ğŸ“Š è¨­å®š - ãƒ¡ãƒ¢ãƒªåˆ¶é™: {memory_limit/1024/1024:.0f}MB, ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
        
        # åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’è¨˜éŒ²
        self._log_memory_usage("é–‹å§‹æ™‚")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®ãƒã‚§ãƒƒã‚¯
        if not force and MarginTradingData.objects.filter(date=target_date).exists():
            self.stdout.write(
                self.style.WARNING(f'{target_date} ã®ãƒ‡ãƒ¼ã‚¿ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚--force ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§å¼·åˆ¶æ›´æ–°å¯èƒ½ã§ã™ã€‚')
            )
            return
        
        # PDF URLç”Ÿæˆ
        pdf_url = self._generate_pdf_url(target_date)
        
        try:
            # ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ»å‡¦ç†
            records_count = self._import_data(pdf_url, target_date, force)
            
            # ãƒ­ã‚°è¨˜éŒ²ï¼ˆæˆåŠŸï¼‰
            DataImportLog.objects.create(
                date=target_date,
                status='SUCCESS',
                message=f'æ­£å¸¸ã« {records_count} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸ',
                records_count=records_count,
                pdf_url=pdf_url
            )
            
            self.stdout.write(
                self.style.SUCCESS(f'âœ… ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†: {target_date} ({records_count}ä»¶)')
            )
            self._log_memory_usage("å®Œäº†æ™‚")
            
        except requests.RequestException as e:
            error_msg = f'PDFå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}'
            self._log_error(target_date, error_msg, pdf_url)
            self.stdout.write(self.style.ERROR(f"âŒ {error_msg}"))
            
        except MemoryError as e:
            error_msg = f'ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼: {str(e)}'
            self._log_error(target_date, error_msg, pdf_url)
            self.stdout.write(self.style.ERROR(f"âŒ {error_msg}"))
            self.stdout.write(self.style.ERROR('ğŸ’¡ å¯¾ç­–: --memory-limit ã‚’å¢—ã‚„ã™ã‹ --batch-size ã‚’å°ã•ãã—ã¦ãã ã•ã„'))
            
        except Exception as e:
            error_msg = f'ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}'
            self._log_error(target_date, error_msg, pdf_url)
            raise CommandError(error_msg)

    def _generate_pdf_url(self, target_date):
        """PDF URLã‚’ç”Ÿæˆ"""
        date_str = target_date.strftime('%Y%m%d')
        base_url = 'https://www.jpx.co.jp/markets/statistics-equities/margin/tvdivq0000001rnl-att/'
        filename = f'syumatsu{date_str}00.pdf'
        return f'{base_url}{filename}'

    def _import_data(self, pdf_url, target_date, force):
        """PDFãƒ‡ãƒ¼ã‚¿ã®å–å¾—ãƒ»å‡¦ç†ï¼ˆè¶…åŠ¹ç‡ç‰ˆï¼‰"""
        # PDFå–å¾—ï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
        self.stdout.write('ğŸ“¥ PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹...')
        response = requests.get(pdf_url, timeout=60, stream=True)
        response.raise_for_status()
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒãƒ£ãƒ³ã‚¯å˜ä½ï¼‰
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            downloaded = 0
            for chunk in response.iter_content(chunk_size=8192):
                tmp_file.write(chunk)
                downloaded += len(chunk)
                if downloaded % (1024*1024) == 0:  # 1MBã”ã¨ã«é€²æ—è¡¨ç¤º
                    self.stdout.write(f'ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {downloaded/1024/1024:.1f}MB')
            tmp_file_path = tmp_file.name
        
        self.stdout.write(f'ğŸ“¥ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {downloaded/1024/1024:.1f}MB')
        self._log_memory_usage("PDFä¿å­˜å¾Œ")
        
        try:
            records_count = self._parse_pdf_ultra_efficient(tmp_file_path, target_date, force)
            return records_count
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            os.unlink(tmp_file_path)

    def _parse_pdf_ultra_efficient(self, pdf_path, target_date, force):
        """PDFè§£æã¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆè¶…åŠ¹ç‡ç‰ˆï¼‰"""
        records_count = 0
        batch_data = []
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã®å‰Šé™¤ï¼ˆforceæ™‚ï¼‰
        if force:
            with transaction.atomic():
                deleted_count = MarginTradingData.objects.filter(date=target_date).delete()[0]
                self.stdout.write(f'ğŸ—‘ï¸  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ {deleted_count} ä»¶ã‚’å‰Šé™¤ã—ã¾ã—ãŸ')
        
        self.stdout.write('ğŸ“„ PDFè§£æé–‹å§‹...')
        
        # PDFã‚’é–‹ã
        pdf_file = None
        try:
            pdf_file = pdfplumber.open(pdf_path)
            total_pages = len(pdf_file.pages)
            self.stdout.write(f'ğŸ“„ ç·ãƒšãƒ¼ã‚¸æ•°: {total_pages}')
            
            for page_num in range(total_pages):
                self.stdout.write(f'ğŸ“„ ãƒšãƒ¼ã‚¸ {page_num + 1}/{total_pages} å‡¦ç†ä¸­...')
                
                # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒšãƒ¼ã‚¸å‡¦ç†å‰ï¼‰
                if self._check_memory_limit():
                    self.stdout.write('ğŸ§  ãƒ¡ãƒ¢ãƒªåˆ¶é™è¿‘ã¥ã - ãƒãƒƒãƒä¿å­˜å®Ÿè¡Œ')
                    if batch_data:
                        self._save_batch(batch_data, target_date)
                        records_count += len(batch_data)
                        batch_data = []
                    self._aggressive_cleanup()
                
                # ãƒšãƒ¼ã‚¸ã‚’å€‹åˆ¥ã«èª­ã¿è¾¼ã¿ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
                page = None
                try:
                    page = pdf_file.pages[page_num]
                    
                    # ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡º
                    tables = page.extract_tables()
                    
                    for table in tables:
                        for row in table:
                            if self._is_data_row(row):
                                try:
                                    data_dict = self._parse_data_row(row, target_date)
                                    batch_data.append(data_dict)
                                    
                                    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«é”ã—ãŸã‚‰ä¿å­˜
                                    if len(batch_data) >= self.batch_size:
                                        self._save_batch(batch_data, target_date)
                                        records_count += len(batch_data)
                                        batch_data = []
                                        
                                        if self.aggressive_gc:
                                            self._aggressive_cleanup()
                                        
                                except Exception as e:
                                    self.stdout.write(f'âš ï¸  è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}')
                                    continue
                    
                    # ãƒšãƒ¼ã‚¸å‡¦ç†å®Œäº†å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                    if (page_num + 1) % self.page_interval == 0:
                        self.stdout.write(f'ğŸ§¹ {page_num + 1}ãƒšãƒ¼ã‚¸å‡¦ç†å®Œäº† - ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ')
                        self._aggressive_cleanup()
                        self._log_memory_usage(f"ãƒšãƒ¼ã‚¸ {page_num + 1} å‡¦ç†å¾Œ")
                    
                except Exception as e:
                    self.stdout.write(f'âš ï¸  ãƒšãƒ¼ã‚¸ {page_num + 1} å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}')
                    continue
                finally:
                    # ãƒšãƒ¼ã‚¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ˜ç¤ºçš„ã«å‰Šé™¤
                    if page:
                        del page
                
                # å°ä¼‘æ­¢ï¼ˆãƒ¡ãƒ¢ãƒªå®‰å®šåŒ–ï¼‰
                time.sleep(0.1)
        
        finally:
            if pdf_file:
                pdf_file.close()
        
        # æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        if batch_data:
            self._save_batch(batch_data, target_date)
            records_count += len(batch_data)
        
        # æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        self._aggressive_cleanup()
        self._log_memory_usage("è§£æå®Œäº†å¾Œ")
        
        return records_count

    def _save_batch(self, batch_data, target_date):
        """ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆDBæ¥ç¶šä¿®æ­£ç‰ˆï¼‰"""
        if not batch_data:
            return
        
        self.stdout.write(f'ğŸ’¾ {len(batch_data)} ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ä¸­...')
        
        # æ¥ç¶šçŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦å¿…è¦ã«å¿œã˜ã¦å†æ¥ç¶š
        try:
            connection.ensure_connection()
        except Exception:
            # æ¥ç¶šãŒåˆ‡ã‚Œã¦ã„ã‚‹å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆDjangoãŒè‡ªå‹•ã§å†æ¥ç¶šï¼‰
            pass
        
        # ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆã¯ã•ã‚‰ã«åˆ†å‰²
        chunk_size = min(len(batch_data), 10)
        
        try:
            with transaction.atomic():
                for data_dict in batch_data:
                    try:
                        # éŠ˜æŸ„ã®å–å¾—ã¾ãŸã¯ä½œæˆ
                        issue, created = MarketIssue.objects.get_or_create(
                            code=data_dict['issue_code'],
                            defaults={
                                'jp_code': data_dict['jp_code'],
                                'name': data_dict['issue_name'],
                                'category': 'B'
                            }
                        )
                        
                        # ä¿¡ç”¨å–å¼•ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆãƒ»æ›´æ–°
                        MarginTradingData.objects.update_or_create(
                            issue=issue,
                            date=target_date,
                            defaults=data_dict['margin_data']
                        )
                        
                    except Exception as e:
                        self.stdout.write(f'âš ï¸  ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {data_dict["issue_code"]} - {str(e)}')
                        continue
                        
        except Exception as e:
            self.stdout.write(f'ğŸš¨ ãƒãƒƒãƒä¿å­˜ã§é‡å¤§ã‚¨ãƒ©ãƒ¼: {str(e)}')
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å€‹åˆ¥ã«ä¿å­˜ã‚’è©¦è¡Œ
            self._save_batch_individually(batch_data, target_date)

    def _parse_data_row(self, row, target_date):
        """ãƒ‡ãƒ¼ã‚¿è¡Œã®è§£æï¼ˆè¾æ›¸å½¢å¼ã§è¿”ã™ï¼‰"""
        first_cell = str(row[0])
        
        # éŠ˜æŸ„æƒ…å ±ã®æŠ½å‡º
        match = re.match(r'B\s+(.+?)\s+æ™®é€šæ ªå¼\s+(\d+)', first_cell)
        if not match:
            raise ValueError(f'éŠ˜æŸ„æƒ…å ±ã®è§£æã«å¤±æ•—: {first_cell}')
        
        issue_name = match.group(1).strip()
        issue_code = match.group(2)
        jp_code = str(row[3]) if row[3] else ''
        
        # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®è§£æ
        numeric_values = []
        for i in range(4, len(row)):
            value = self._parse_numeric_value(row[i])
            numeric_values.append(value)
        
        # è¶³ã‚Šãªã„å€¤ã‚’0ã§åŸ‹ã‚ã‚‹
        while len(numeric_values) < 12:
            numeric_values.append(0)
        
        return {
            'issue_code': issue_code,
            'jp_code': jp_code,
            'issue_name': issue_name,
            'margin_data': {
                'outstanding_sales': numeric_values[0],
                'outstanding_sales_change': numeric_values[1],
                'outstanding_purchases': numeric_values[2],
                'outstanding_purchases_change': numeric_values[3],
                'negotiable_credit': numeric_values[4],
                'negotiable_credit_change': numeric_values[5],
                'standardized_credit': numeric_values[6],
                'standardized_credit_change': numeric_values[7],
                'additional_data_1': numeric_values[8] if len(numeric_values) > 8 else None,
                'additional_data_2': numeric_values[9] if len(numeric_values) > 9 else None,
                'additional_data_3': numeric_values[10] if len(numeric_values) > 10 else None,
                'additional_data_4': numeric_values[11] if len(numeric_values) > 11 else None,
            }
        }

    def _is_data_row(self, row):
        """ãƒ‡ãƒ¼ã‚¿è¡Œã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        if not row or len(row) < 4:
            return False
        
        first_cell = str(row[0]) if row[0] else ''
        return (first_cell.startswith('B ') and 
                'æ™®é€šæ ªå¼' in first_cell and 
                len(row) >= 10)

    def _parse_numeric_value(self, value):
        """æ•°å€¤ã®è§£æï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã€â–²ãƒã‚¤ãƒŠã‚¹è¨˜å·å¯¾å¿œï¼‰"""
        if not value or value == '-':
            return 0
        
        value_str = str(value).strip()
        
        # â–²ãƒã‚¤ãƒŠã‚¹è¨˜å·ã®å‡¦ç†
        is_negative = value_str.startswith('â–²')
        if is_negative:
            value_str = value_str[1:]
        
        # ã‚«ãƒ³ãƒã‚’é™¤å»ã—ã¦æ•°å€¤å¤‰æ›
        try:
            value_str = value_str.replace(',', '')
            numeric_value = int(float(value_str))
            return -numeric_value if is_negative else numeric_value
        except (ValueError, TypeError):
            return 0

    def _check_memory_limit(self):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒåˆ¶é™ã‚’è¶…ãˆã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        if not PSUTIL_AVAILABLE:
            return False
        
        try:
            process = psutil.Process()
            memory_usage = process.memory_info().rss
            return memory_usage > self.memory_limit * 0.8  # 80%ã§è­¦å‘Š
        except:
            return False

    def _log_memory_usage(self, label):
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        if not PSUTIL_AVAILABLE:
            self.stdout.write(f'{label}: ãƒ¡ãƒ¢ãƒªç›£è¦–ä¸å¯ (psutilæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«)')
            return
        
        try:
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            limit_mb = self.memory_limit / 1024 / 1024
            usage_percent = (memory_mb / limit_mb) * 100
            
            status = "ğŸŸ¢" if usage_percent < 60 else "ğŸŸ¡" if usage_percent < 80 else "ğŸ”´"
            self.stdout.write(f'{status} {label}: {memory_mb:.1f}MB ({usage_percent:.1f}%)')
        except Exception as e:
            self.stdout.write(f'{label}: ãƒ¡ãƒ¢ãƒªç›£è¦–ã‚¨ãƒ©ãƒ¼ {e}')

    def _aggressive_cleanup(self):
        """ç©æ¥µçš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆDBæ¥ç¶šå•é¡Œä¿®æ­£ç‰ˆï¼‰"""
        # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        gc.collect()
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®ã‚¯ãƒªã‚¢ï¼ˆæ¥ç¶šãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        try:
            if connection.connection is not None:
                connection.close()
        except Exception:
            # æ¥ç¶šé–¢é€£ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç„¡è¦–
            pass
        
        # å°‘ã—å¾…æ©Ÿï¼ˆã‚·ã‚¹ãƒ†ãƒ ãŒãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã™ã‚‹æ™‚é–“ã‚’ä¸ãˆã‚‹ï¼‰
        time.sleep(0.5)

    def _save_batch_individually(self, batch_data, target_date):
        """å€‹åˆ¥ä¿å­˜ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        self.stdout.write('ğŸ”„ å€‹åˆ¥ä¿å­˜ãƒ¢ãƒ¼ãƒ‰ã§å†è©¦è¡Œ...')
        success_count = 0
        
        for data_dict in batch_data:
            try:
                # æ–°ã—ã„ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã§å€‹åˆ¥ã«ä¿å­˜
                with transaction.atomic():
                    issue, created = MarketIssue.objects.get_or_create(
                        code=data_dict['issue_code'],
                        defaults={
                            'jp_code': data_dict['jp_code'],
                            'name': data_dict['issue_name'],
                            'category': 'B'
                        }
                    )
                    
                    MarginTradingData.objects.update_or_create(
                        issue=issue,
                        date=target_date,
                        defaults=data_dict['margin_data']
                    )
                    success_count += 1
                    
            except Exception as e:
                self.stdout.write(f'âŒ å€‹åˆ¥ä¿å­˜ã‚‚å¤±æ•—: {data_dict["issue_code"]} - {str(e)}')
                continue
        
        self.stdout.write(f'âœ… å€‹åˆ¥ä¿å­˜ã§ {success_count}/{len(batch_data)} ä»¶æˆåŠŸ')

    def _log_error(self, target_date, error_msg, pdf_url):
        """ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®è¨˜éŒ²"""
        try:
            DataImportLog.objects.create(
                date=target_date,
                status='FAILED',
                message=error_msg,
                pdf_url=pdf_url
            )
        except:
            pass  # ãƒ­ã‚°è¨˜éŒ²ã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¦ã‚‚å‡¦ç†ã¯ç¶šè¡Œ